{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import nan\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\",trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset/train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[['parent_comment','comment','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a41dfac88>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.countplot(x='label',hue='label',data=df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19401</th>\n",
       "      <td>This is by far the best thing I've ever seen h...</td>\n",
       "      <td>Well, if the statistics are anything to go by...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411703</th>\n",
       "      <td>Fire in the sky. Holy shit that movie was inte...</td>\n",
       "      <td>If you like Aliens abductions movies try \"The ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179991</th>\n",
       "      <td>Reddit comments. I'll upvote just about anythi...</td>\n",
       "      <td>aparently you have low standards for guitars too</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196427</th>\n",
       "      <td>If I get 100 upvotes, I'll buy everyone who up...</td>\n",
       "      <td>Just in Case</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257764</th>\n",
       "      <td>Not everyone lives in the US.</td>\n",
       "      <td>You're only truly living when you're an American.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           parent_comment  \\\n",
       "19401   This is by far the best thing I've ever seen h...   \n",
       "411703  Fire in the sky. Holy shit that movie was inte...   \n",
       "179991  Reddit comments. I'll upvote just about anythi...   \n",
       "196427  If I get 100 upvotes, I'll buy everyone who up...   \n",
       "257764                      Not everyone lives in the US.   \n",
       "\n",
       "                                                  comment  label  \n",
       "19401    Well, if the statistics are anything to go by...      1  \n",
       "411703  If you like Aliens abductions movies try \"The ...      0  \n",
       "179991   aparently you have low standards for guitars too      1  \n",
       "196427                                       Just in Case      0  \n",
       "257764  You're only truly living when you're an American.      1  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    tokens_wo_stopwords = []\n",
    "    for i in range(0,len(tokens)):\n",
    "        if tokens[i].lower() not in stop_words:\n",
    "            tokens_wo_stopwords.append(tokens[i].lower())\n",
    "    return tokens_wo_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(token):\n",
    "    pos_tag = nltk.pos_tag([token])[0][1]\n",
    "    if pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(0,len(tokens)):\n",
    "        tokens[i] = lemmatizer.lemmatize(tokens[i],pos=str(get_pos_tag(tokens[i])))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dictionary(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in dictionary:\n",
    "            dictionary.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dictionary():\n",
    "    with open('data/processed/dictionary.txt','w') as file:\n",
    "        file.writelines(\"%s\\n\" % word for word in dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dictionary():\n",
    "    with open('data/processed/dictionary.txt','r') as file:\n",
    "        temp = file.read().splitlines()\n",
    "        for i in range(0,len(temp)):\n",
    "            dictionary.append(temp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(sess,path):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(sess,path):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    processed_sentence = re.sub(r'[^a-zA-Z]', ' ', sentence)\n",
    "    tokens_comment = word_tokenize(processed_sentence)\n",
    "    tokens_comment = remove_stopwords(tokens_comment)\n",
    "    tokens_comment = lemmatize(tokens_comment)\n",
    "    return tokens_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(dataset):\n",
    "    for index,row in dataset.iterrows():\n",
    "        tokens_comment = preprocess(str(row['parent_comment']) + \" \" + str(row['comment']))\n",
    "        add_to_dictionary(tokens_comment)\n",
    "    save_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_dictionary():\n",
    "    if not os.path.isfile('data/processed/dictionary.txt'):\n",
    "        starttime = time.time()\n",
    "        create_dictionary(df_new)\n",
    "        endtime = time.time()\n",
    "        print(\"Time to create dictionary\")\n",
    "        print(endtime - starttime)\n",
    "    else:   \n",
    "        read_dictionary()\n",
    "    print(\"Length of dictionary:- \")\n",
    "    print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_embeddings_dict():\n",
    "    starttime = time.time()\n",
    "    with open('data/processed/glove.6B.300d.txt','r') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            word_embedding = np.asarray(values[1:])\n",
    "            embeddings[word] = word_embedding\n",
    "    endtime = time.time()\n",
    "    print(\"Time taken to load embeddings:- \")\n",
    "    print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tokens(tokens,max_length):\n",
    "    zeros = np.zeros(len(tokens[0]))\n",
    "    while len(tokens) < max_length:\n",
    "        tokens = np.vstack([tokens,zeros])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(X):\n",
    "    max_length = 0\n",
    "    index = 0\n",
    "    for i in range(0,len(X.index)):\n",
    "        if(X[i:i+1][X.index[i]] is not nan):\n",
    "            preprocessed_tokens = preprocess(X[i:i+1][X.index[i]])\n",
    "            if(len(preprocessed_tokens) < 30):\n",
    "                if max_length < len(preprocessed_tokens):\n",
    "                    max_length = len(preprocessed_tokens)\n",
    "                    index = i\n",
    "    print(index)\n",
    "    print(preprocess(X[index:index+1][X.index[index]]))\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup(x,embedding_dim=300):\n",
    "    if(len(embeddings) == 0):\n",
    "        populate_embeddings_dict()\n",
    "    embedding = []\n",
    "    for i in range(0,len(x)):\n",
    "        if(x[i] in embeddings):\n",
    "            embedding.append(embeddings[x[i]])\n",
    "        else:\n",
    "            zero_arr = np.zeros(embedding_dim).tolist()\n",
    "            embedding.append(zero_arr)\n",
    "    return np.array(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elmo_embeddings(sess,tokens_input,tokens_length):\n",
    "    embeddings = elmo(inputs={\"tokens\": tokens_input,\"sequence_len\": tokens_length},signature='tokens',as_dict=True)[\"elmo\"]\n",
    "    return sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deep_contextualized_embeddings(X,y,max_length):\n",
    "    deep_contextualized_embeddings = []\n",
    "    sequence_lengths = []\n",
    "    elmo_tokens = []\n",
    "    elmo_tokens_length = []\n",
    "    elmo_embeddings_list = []\n",
    "    y_pred = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        starttime = time.time()\n",
    "        for i in range(0,len(X.index)):\n",
    "            if(X[i:i+1][X.index[i]] is not nan):\n",
    "                preprocessed_tokens = preprocess(X[i:i+1][X.index[i]])\n",
    "                if(len(preprocessed_tokens) < 30):\n",
    "                    sequence_lengths.append(len(preprocessed_tokens))\n",
    "                    y_pred.append(y[i:i+1][y.index[i]])\n",
    "                    for j in range(len(preprocessed_tokens),max_length):\n",
    "                        preprocessed_tokens.append(\"<PAD>\")\n",
    "                    #word_embedding = embedding_lookup(preprocessed_tokens)\n",
    "                    #word_embedding = np.array(pad_tokens(word_embedding,max_length))\n",
    "                    elmo_tokens.append(preprocessed_tokens)\n",
    "                    elmo_tokens_length.append(len(preprocessed_tokens))\n",
    "                    #deep_contextualized_embeddings.append(np.hstack([word_embedding,elmo_embedding]))\n",
    "                    if (i + 1) % 1000 == 0:\n",
    "                        elmo_embedding = get_elmo_embeddings(sess,np.array(elmo_tokens),np.array(elmo_tokens_length))\n",
    "                        for j in range(0,len(elmo_embedding)):\n",
    "                            deep_contextualized_embeddings.append(np.array(pad_tokens(elmo_embedding[j],max_length)))\n",
    "                        temp_arr = np.array(deep_contextualized_embeddings)\n",
    "                        print(temp_arr.shape)\n",
    "                        elmo_tokens.clear()\n",
    "                        elmo_tokens_length.clear()\n",
    "        endtime = time.time()\n",
    "        print(\"Total time to generate embeddings:- \")\n",
    "        print(endtime - starttime)\n",
    "    return np.array(deep_contextualized_embeddings),np.array(y_pred),np.array(sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Remove nan here\n",
    "X = df_new['comment']\n",
    "y = df_new['label']\n",
    "X.reset_index()\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000,)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 422802,  223003,  451567,  656649,  589803,   71789,  449464,\n",
       "             135417,   79455,  613154,\n",
       "            ...\n",
       "             209188,  609738,   66043,  978323, 1008612,  277429,  503838,\n",
       "             310373,  355943,  490081],\n",
       "           dtype='int64', length=8000)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([250463, 562748, 259141,   3548, 948929, 893553, 695631,  19401,\n",
       "             89392, 221872,\n",
       "            ...\n",
       "            528435, 183308, 994576, 855400, 353617, 595437, 260140, 588572,\n",
       "            896236, 678411],\n",
       "           dtype='int64', length=2000)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1419\n",
      "['cho', 'second', 'silence', 'absurd', 'true', 'damage', 'lulu', 'extreme', 'cockblock', 'try', 'kill', 'anybody', 'ghoul', 'hard', 'target', 'even', 'aa', 'move', 'spam', 'probably', 'disagree', 'sake', 'since', 'root', 'clg', 'c', 'look', 'text', 'flair']\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 15:23:41.920796 4432184768 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(998, 29, 1024)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 15:27:30.651120 4432184768 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1997, 29, 1024)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 15:31:33.399988 4432184768 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2997, 29, 1024)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 15:36:48.110479 4432184768 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3993, 29, 1024)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 15:42:03.230631 4432184768 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4989, 29, 1024)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 15:49:32.514594 4432184768 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5986, 29, 1024)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 15:54:27.443722 4432184768 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6984, 29, 1024)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 16:01:13.930114 4432184768 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7981, 29, 1024)\n",
      "Total time to generate embeddings:- \n",
      "2686.6201231479645\n"
     ]
    }
   ],
   "source": [
    "deep_contextualized_embeddings_train,y_pred_train,sequence_lengths_train = get_deep_contextualized_embeddings(X_train,y_train,get_max_length(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n",
      "['wow', 'player', 'peasant', 'thats', 'really', 'good', 'argument', 'call', 'steal', 'reference', 'scummy', 'fuck', 'reference', 'include', 'change', 'point', 'recognizable', 'still', 'uniqueness', 'like', 'dark', 'soul', 'reference', 'berserk', 'without', 'directly', 'put', 'gut']\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 16:08:47.510442 4432184768 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(998, 28, 1024)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 16:14:22.938108 4432184768 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1994, 28, 1024)\n",
      "Total time to generate embeddings:- \n",
      "660.1957371234894\n"
     ]
    }
   ],
   "source": [
    "deep_contextualized_embeddings_test,y_pred_test,sequence_lengths_test = get_deep_contextualized_embeddings(X_test,y_test,get_max_length(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_contextualized_embeddings_train = np.array(deep_contextualized_embeddings_train)\n",
    "deep_contextualized_embeddings_test = np.array(deep_contextualized_embeddings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7981, 29, 1024)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_contextualized_embeddings_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1994, 28, 1024)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_contextualized_embeddings_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM():\n",
    "    \n",
    "    def __init__(self,num_classes,elmo_embed_size,embed_size,batch_size,epochs,learning_rate):\n",
    "        self.X = tf.placeholder(shape=[None,None,embed_size + elmo_embed_size],dtype=tf.float32,name='X')\n",
    "        self.y = tf.placeholder(shape=[None],dtype=tf.int64,name='y')\n",
    "        self.sequence_lengths = tf.placeholder(shape=[None],dtype=tf.int32,name='sequence_lengths')\n",
    "        self.num_classes = num_classes\n",
    "        self.elmo_embed_size = elmo_embed_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = elmo_embed_size + embed_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model()\n",
    "    \n",
    "    def model(self):\n",
    "        cell_fw = tf.contrib.rnn.LSTMCell(self.hidden_size,forget_bias=1.0,state_is_tuple=True,reuse=tf.get_variable_scope().reuse)\n",
    "        cell_bw = tf.contrib.rnn.LSTMCell(self.hidden_size,forget_bias=1.0,state_is_tuple=True,reuse=tf.get_variable_scope().reuse)\n",
    "        with tf.variable_scope('Bi-Directional-LSTM',reuse=tf.AUTO_REUSE):\n",
    "            output_vals,output_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw = cell_fw,\n",
    "            cell_bw = cell_bw,\n",
    "            inputs = self.X,\n",
    "            sequence_length = self.sequence_lengths,\n",
    "            dtype = tf.float32)\n",
    "        self.final_state = tf.concat([output_states[0].c,output_states[1].c],axis=1)\n",
    "        with tf.variable_scope('Softmax',reuse=tf.AUTO_REUSE):\n",
    "            self.softmax_w = tf.get_variable('softmax_w',shape=[2 * self.hidden_size,self.num_classes],initializer=tf.truncated_normal_initializer(),dtype=tf.float32)\n",
    "            self.softmax_b = tf.get_variable('softmax_b',shape=[self.num_classes],initializer=tf.constant_initializer(0.0),dtype=tf.float32)\n",
    "        self.logits = tf.matmul(self.final_state,self.softmax_w) + self.softmax_b\n",
    "        self.predictions = tf.argmax(tf.nn.softmax(self.logits),1,name='predictions')\n",
    "        self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y,logits=self.logits)\n",
    "        self.cost = tf.reduce_mean(self.cross_entropy)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.predictions,self.y),tf.float32),name='accuracy')\n",
    "    \n",
    "    def train(self,X_train,y_train,sequence_lengths_train,path):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.train_step = optimizer.minimize(self.cost)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for j in range(0,self.epochs):\n",
    "                starttime = time.time()\n",
    "                epoch_cost = 0\n",
    "                for i in range(0,math.ceil(X_train.shape[0]/self.batch_size)):\n",
    "                    X_train_batch = X_train[i * self.batch_size : min((i + 1) * self.batch_size,X_train.shape[0])]\n",
    "                    y_train_batch = y_train[i * self.batch_size : min((i + 1) * self.batch_size,len(y_train))]\n",
    "                    sequence_lengths_batch = sequence_lengths_train[i * self.batch_size : min((i + 1) * self.batch_size,len(sequence_lengths_train))]\n",
    "                    fetches = {\n",
    "                        'cross_entropy': self.cross_entropy,\n",
    "                        'cost': self.cost,\n",
    "                        'train_step': self.train_step\n",
    "                    }\n",
    "                    feed_dict = {\n",
    "                        self.X : X_train_batch,\n",
    "                        self.y : y_train_batch,\n",
    "                        self.sequence_lengths : sequence_lengths_batch\n",
    "                    }\n",
    "                    resp = sess.run(fetches,feed_dict)\n",
    "                    epoch_cost += resp['cost']\n",
    "                endtime = time.time()\n",
    "                print('Time to train epoch ' + str(j) + ':-')\n",
    "                print(endtime - starttime)\n",
    "                print('Epoch ' + str(j) + \" cost :-\")\n",
    "                print(epoch_cost)\n",
    "            save_model(sess,path)\n",
    "            \n",
    "    def test(self,X_test,y_test,sequence_lengths_test,path):\n",
    "        with tf.Session() as sess:\n",
    "            starttime = time.time()\n",
    "            load_model(sess,path)\n",
    "            fetches = {\n",
    "                'accuracy': self.accuracy,\n",
    "                'predictions': self.predictions\n",
    "            }\n",
    "            feed_dict = {\n",
    "                self.X : X_test,\n",
    "                self.y : y_test,\n",
    "                self.sequence_lengths: sequence_lengths_test\n",
    "            }\n",
    "            resp = sess.run(fetches,feed_dict)\n",
    "            endtime = time.time()\n",
    "            print('Time to test model:- ')\n",
    "            print(endtime - starttime)\n",
    "            print('Model accuracy:- ')\n",
    "            print(resp['accuracy'])\n",
    "            print('Model predictions:- ')\n",
    "            print(resp['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters for the model\n",
    "#Hyperparameter tuning required\n",
    "num_classes = 2\n",
    "word_embedding_size = 0 #For now as we are currently not using Glove\n",
    "elmo_embedding_size = 1024\n",
    "batch_size = 500\n",
    "epochs = 5 #To be increased as the size of the dataset increases(current size being considered:- 10000 data points (8000 - Train,2000 - Test))\n",
    "learning_rate = 0.2 #To be changed to exponentially decreasing value based on epochs passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Currenlty not using concatenation of Glove with ELMO\n",
    "tf.reset_default_graph()\n",
    "lstm = LSTM(num_classes,word_embedding_size,elmo_embedding_size,batch_size,epochs,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm.train(deep_contextualized_embeddings_train,y_pred_train,sequence_lengths_train,'data/trained_models/elmo_bi_directional_lstm.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.test(deep_contextualized_embeddings_test,y_pred_test,sequence_lengths_test,'data/trained_models/elmo_bi_directional_lstm.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/trained/generated_embeddings.npy',deep_contextualized_embeddings_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

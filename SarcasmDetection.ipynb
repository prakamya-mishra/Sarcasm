{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashvatkedia/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0414 15:30:41.755306 4704441792 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import nan\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\",trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset/train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[['parent_comment','comment','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a4a7245c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEKCAYAAADEovgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAFZVJREFUeJzt3X+wnmV95/H3l/wwWkAgOSLmhCZKsA1stST8kHZYhUoCVYIWMfQHQQJxxrBLa2cX3NmVFYur01YK/mBLJSV0mcRIbQkIyWZQdOzKj6AIJCybI2BzIpCQIEFtIAnf/eO5Ao/xnCcn4VznTs55v2aeOff9va/7vr5hMnzmvs+V+4nMRJKkmg5ougFJ0vBn2EiSqjNsJEnVGTaSpOoMG0lSdYaNJKk6w0aSVJ1hI0mqzrCRJFU3uukG9hUTJkzIyZMnN92GJO1XHnjggWczs2t34wybYvLkyaxatarpNiRpvxIRPx7IOB+jSZKqM2wkSdUZNpKk6vydjSQ1aNu2bfT29rJ169amW+lo3LhxdHd3M2bMmL0637CRpAb19vZy0EEHMXnyZCKi6Xb6lJls2rSJ3t5epkyZslfX8DGaJDVo69atjB8/fp8NGoCIYPz48a/p7suwkaSG7ctBs9Nr7bFq2ETEkxHxcEQ8GBGrSu2wiFgZEWvLz0NLPSLi2ojoiYiHIuK4tuvMLePXRsTctvr0cv2ecm50mkOS1IyhuLN5T2a+MzNnlP3LgbsycypwV9kHOAOYWj7zgeugFRzAFcCJwAnAFW3hcR1wcdt5s3YzhyTtlw488MCOx5988kmOPfbYPbrmBRdcwC233PJa2hqwJhYIzAbeXbYXAXcDl5X6TZmZwD0RcUhEHFHGrszMzQARsRKYFRF3Awdn5j2lfhNwNnBnhzmqmv6fbqo9xX7jgb88v+kWpD7965X/rukWfsn29/4NL/7k5d0PzJd58Ser+z384jPrye0vdhzTpNp3Ngn874h4ICLml9rhmflU2X4aOLxsTwTWtZ3bW2qd6r191DvNIUn7tZ/9/BfMOnceJ838ENNP+wC3rfjmK8e2b9/B3Esu4x3//v2cd/Gf8Yt/+zcAvv/Qan7vDy7gXbPO5X1/OJ+nntk45H3XDpvfzczjaD0iWxARp7QfLHcxWbOBTnNExPyIWBURqzZuHPr/+JK0p8a9bixLb7iGe1Z8jRVfW8hlV/4lrf/Nwf/70RN8dO6H+eG3b+Ogg36Nv120hG3btvHx//oZFl//eb63fClzP/wBrvjcNUPed9XHaJm5vvzcEBH/ROt3Ls9ExBGZ+VR5TLahDF8PTGo7vbvU1vPqI7Gd9btLvbuP8XSYY9f+rgeuB5gxY0bV0JOkwZCZfPKz1/Dde1dxQBzAT57ewDMbNwHQ/ZY3c/LxrbVV533w/Xx54c2c/u7fZfVjPfz+nIsB2PHyy7z5TROGvO9qYRMRvwYckJkvlO3TgSuBZcBc4LPl563llGXAJRGxhNZigOdLWKwAPtO2KOB04BOZuTkitkTEScC9wPnAF9qu1dcckrRfW/z1b/Dsps18786ljBkzhqNPPJ2tL74I/Ory5IhWOE07+ii+fdvNTbT7ipqP0Q4HvhsRPwTuA76RmctpBcB7I2It8HtlH+AO4HGgB/g74GMAZWHAp4H7y+fKnYsFypivlHN+RGtxAB3mkKT92pYXXqBrwnjGjBnD3f9yH//a+5NXjq1b/xT3rHoQgK/+8zc4+fjjOPptU9i4efMr9W3btrHmsZ4h77vanU1mPg68o4/6JuC0PuoJLOjnWguBhX3UVwG/stavvzkkaX8354Pv4w/mXsL00z7Acb91DG8/6tXXxxz9tin8z0WL+eif/zd+8+i3MX/uhxk7dgyL//ZqPv7J/8GWLS+wfccOLrnoT5j29qOGtG/fjSZJ+4FNa+8HYMJhh/b7SOyh79zWZ/0dx/4Gd3190a/Ub7zxxkHrb3d8XY0kqTrDRpJUnWEjSarOsJEkVWfYSJKqM2wkSdW59FmS9iEnX/3AoF7v//zZ9AGNW758OZdeeik7duzgoosu4vLLB/ebWbyzkaQRbseOHSxYsIA777yTNWvWsHjxYtasWTOocxg2kjTC3XfffRx11FG89a1vZezYscyZM4dbbx3cV0oaNpI0wq1fv55Jk1596X53dzfr16/vcMaeM2wkSdUZNpI0wk2cOJF16179QuTe3l4mTpzY4Yw9Z9hI0gh3/PHHs3btWp544gleeukllixZwllnnTWoc7j0WZL2IQNdqjyYRo8ezRe/+EVmzpzJjh07uPDCCznmmGMGd45BvZokab905plncuaZZ1a7vo/RJEnVGTaSpOoMG0lSdYaNJKk6w0aSVJ1hI0mqzqXPkrQPeeYrcwb1eodftGS3Yy688EJuv/123vSmN/HII48M6vw7eWcjSSPcBRdcwPLly6vOYdhI0gh3yimncNhhh1Wdw7CRJFVn2EiSqjNsJEnVGTaSpOpc+ixJ+5CBLFUebOeddx533303zz77LN3d3XzqU59i3rx5gzqHYSNJI9zixYurz+FjNElSddXDJiJGRcQPIuL2sj8lIu6NiJ6I+GpEjC3115X9nnJ8cts1PlHqj0XEzLb6rFLriYjL2+p9ziFJasZQ3NlcCjzatv854OrMPAp4Dtj5YHAe8FypX13GERHTgDnAMcAs4MslwEYBXwLOAKYB55WxneaQpH1MkplNN7Fbr7XHqmETEd3A7wNfKfsBnArcUoYsAs4u27PLPuX4aWX8bGBJZr6YmU8APcAJ5dOTmY9n5kvAEmD2buaQpH3KqC3r+OnPX9qnAycz2bRpE+PGjdvra9ReIPA3wH8GDir744GfZub2st8LTCzbE4F1AJm5PSKeL+MnAve0XbP9nHW71E/czRyStE95ww/+js1czMaDJwExpHOPfn7g9xvjxo2ju7t77+fa6zN3IyLeB2zIzAci4t215nktImI+MB/gyCOPbLgbSSPRAS+9wIH3fr6RuY/85MNDNlfNx2i/A5wVEU/SesR1KnANcEhE7Ay5bmB92V4PTAIox98IbGqv73JOf/VNHeb4JZl5fWbOyMwZXV1de/8nlSR1VC1sMvMTmdmdmZNp/YL/m5n5R8C3gHPKsLnArWV7WdmnHP9mth5iLgPmlNVqU4CpwH3A/cDUsvJsbJljWTmnvzkkSQ1o4t/ZXAZ8PCJ6aP1+5YZSvwEYX+ofBy4HyMzVwFJgDbAcWJCZO8rvZC4BVtBa7ba0jO00hySpAUPyBoHMvBu4u2w/Tmsl2a5jtgIf6uf8q4Cr+qjfAdzRR73POSRJzfANApKk6gwbSVJ1ho0kqTrDRpJUnWEjSarOsJEkVWfYSJKqM2wkSdUZNpKk6gwbSVJ1ho0kqTrDRpJUnWEjSarOsJEkVWfYSJKqM2wkSdUZNpKk6gwbSVJ1ho0kqTrDRpJUnWEjSarOsJEkVWfYSJKqM2wkSdUZNpKk6gwbSVJ1ho0kqTrDRpJUnWEjSarOsJEkVWfYSJKqM2wkSdVVC5uIGBcR90XEDyNidUR8qtSnRMS9EdETEV+NiLGl/rqy31OOT2671idK/bGImNlWn1VqPRFxeVu9zzkkSc2oeWfzInBqZr4DeCcwKyJOAj4HXJ2ZRwHPAfPK+HnAc6V+dRlHREwD5gDHALOAL0fEqIgYBXwJOAOYBpxXxtJhDklSA6qFTbb8rOyOKZ8ETgVuKfVFwNlle3bZpxw/LSKi1Jdk5ouZ+QTQA5xQPj2Z+XhmvgQsAWaXc/qbQ5LUgKq/syl3IA8CG4CVwI+An2bm9jKkF5hYticC6wDK8eeB8e31Xc7prz6+wxySpAZUDZvM3JGZ7wS6ad2J/EbN+fZURMyPiFURsWrjxo1NtyNJw9aQrEbLzJ8C3wLeBRwSEaPLoW5gfdleD0wCKMffCGxqr+9yTn/1TR3m2LWv6zNzRmbO6Orqek1/RklS/wYUNhFx10BquxzviohDyvbrgfcCj9IKnXPKsLnArWV7WdmnHP9mZmapzymr1aYAU4H7gPuBqWXl2VhaiwiWlXP6m0OS1IDRnQ5GxDjgDcCEiDgUiHLoYHb/e5AjgEVl1dgBwNLMvD0i1gBLIuIvgB8AN5TxNwD/EBE9wGZa4UFmro6IpcAaYDuwIDN3lP4uAVYAo4CFmbm6XOuyfuaQJDWgY9gAHwX+FHgL8ACvhs0W4IudTszMh4Df7qP+OK3f3+xa3wp8qJ9rXQVc1Uf9DuCOgc4hSWpGx7DJzGuAayLiP2TmF4aoJ0nSMLO7OxsAMvMLEXEyMLn9nMy8qVJfkqRhZEBhExH/ALwNeBDYUcoJGDaSpN0aUNgAM4BpZaWXJEl7ZKD/zuYR4M01G5EkDV8DvbOZAKyJiPtovWATgMw8q0pXkqRhZaBh899rNiFJGt4Guhrt27UbkSQNXwNdjfYCrdVnAGNpfV3AzzPz4FqNSZKGj4He2Ry0c7vtO2ZOqtWUJGl42eO3PpcvRftnYOZuB0uSxMAfo32wbfcAWv/uZmuVjiRJw85AV6O9v217O/AkrUdpkiTt1kB/Z/OR2o1IkoavgX55WndE/FNEbCiff4yI7trNSZKGh4EuEPh7Wt+Y+Zbyua3UJEnarYGGTVdm/n1mbi+fG4Guin1JkoaRgYbNpoj444gYVT5/DGyq2ZgkafgYaNhcCJwLPA08BZwDXFCpJ0nSMDPQpc9XAnMz8zmAiDgM+CtaISRJUkcDvbP5rZ1BA5CZm4HfrtOSJGm4GWjYHBARh+7cKXc2A70rkiSNcAMNjL8GvhcRXyv7HwKuqtOSJGm4GegbBG6KiFXAqaX0wcxcU68tSdJwMuBHYSVcDBhJ0h7b468YkCRpTxk2kqTqDBtJUnWGjSSpOsNGklSdYSNJqs6wkSRVVy1sImJSRHwrItZExOqIuLTUD4uIlRGxtvw8tNQjIq6NiJ6IeCgijmu71twyfm1EzG2rT4+Ih8s510ZEdJpDktSMmnc224E/z8xpwEnAgoiYBlwO3JWZU4G7yj7AGcDU8pkPXAevvIftCuBE4ATgirbwuA64uO28WaXe3xySpAZUC5vMfCozv1+2XwAeBSYCs4FFZdgi4OyyPRu4KVvuAQ6JiCOAmcDKzNxc3jy9EphVjh2cmfdkZgI37XKtvuaQJDVgSH5nExGTaX0lwb3A4Zn5VDn0NHB42Z4IrGs7rbfUOtV7+6jTYQ5JUgOqh01EHAj8I/Cnmbml/Vi5I8ma83eaIyLmR8SqiFi1cePGmm1I0ohWNWwiYgytoLk5M79eys+UR2CUnxtKfT0wqe307lLrVO/uo95pjl+Smddn5ozMnNHV1bV3f0hJ0m7VXI0WwA3Ao5n5+bZDy4CdK8rmAre21c8vq9JOAp4vj8JWAKdHxKFlYcDpwIpybEtEnFTmOn+Xa/U1hySpATW/bfN3gD8BHo6IB0vtvwCfBZZGxDzgx8C55dgdwJlAD/AL4CPQ+grqiPg0cH8Zd2X5WmqAjwE3Aq8H7iwfOswhSWpAtbDJzO8C0c/h0/oYn8CCfq61EFjYR30VcGwf9U19zSFJaoZvEJAkVWfYSJKqM2wkSdUZNpKk6gwbSVJ1ho0kqTrDRpJUnWEjSarOsJEkVWfYSJKqM2wkSdUZNpKk6gwbSVJ1ho0kqTrDRpJUnWEjSarOsJEkVWfYSJKqM2wkSdUZNpKk6gwbSVJ1ho0kqTrDRpJUnWEjSarOsJEkVWfYSJKqM2wkSdUZNpKk6gwbSVJ1ho0kqTrDRpJUnWEjSaquWthExMKI2BARj7TVDouIlRGxtvw8tNQjIq6NiJ6IeCgijms7Z24ZvzYi5rbVp0fEw+WcayMiOs0hSWpOzTubG4FZu9QuB+7KzKnAXWUf4AxgavnMB66DVnAAVwAnAicAV7SFx3XAxW3nzdrNHJKkhlQLm8z8DrB5l/JsYFHZXgSc3Va/KVvuAQ6JiCOAmcDKzNycmc8BK4FZ5djBmXlPZiZw0y7X6msOSVJDhvp3Nodn5lNl+2ng8LI9EVjXNq631DrVe/uod5pDktSQxhYIlDuSbHKOiJgfEasiYtXGjRtrtiJJI9pQh80z5REY5eeGUl8PTGob111qnerdfdQ7zfErMvP6zJyRmTO6urr2+g8lSepsqMNmGbBzRdlc4Na2+vllVdpJwPPlUdgK4PSIOLQsDDgdWFGObYmIk8oqtPN3uVZfc0iSGjK61oUjYjHwbmBCRPTSWlX2WWBpRMwDfgycW4bfAZwJ9AC/AD4CkJmbI+LTwP1l3JWZuXPRwcdorXh7PXBn+dBhDklSQ6qFTWae18+h0/oYm8CCfq6zEFjYR30VcGwf9U19zSFJao5vEJAkVWfYSJKqM2wkSdUZNpKk6gwbSVJ1ho0kqTrDRpJUnWEjSarOsJEkVWfYSJKqM2wkSdUZNpKk6gwbSVJ1ho0kqTrDRpJUnWEjSarOsJEkVWfYSJKqM2wkSdUZNpKk6gwbSVJ1ho0kqTrDRpJUnWEjSarOsJEkVWfYSJKqM2wkSdUZNpKk6gwbSVJ1ho0kqTrDRpJUnWEjSapu2IZNRMyKiMcioiciLm+6H0kayYZl2ETEKOBLwBnANOC8iJjWbFeSNHINy7ABTgB6MvPxzHwJWALMbrgnSRqxhmvYTATWte33lpokqQGjm26gSRExH5hfdn8WEY812c9wEn81dwLwbNN9SH3w7+ZOV8RgXOXXBzJouIbNemBS2353qf2SzLweuH6omhpJImJVZs5oug9pV/7dbMZwfYx2PzA1IqZExFhgDrCs4Z4kacQalnc2mbk9Ii4BVgCjgIWZubrhtiRpxBqWYQOQmXcAdzTdxwjm40ntq/y72YDIzKZ7kCQNc8P1dzaSpH2IYaNB5WuCtK+KiIURsSEiHmm6l5HIsNGg8TVB2sfdCMxquomRyrDRYPI1QdpnZeZ3gM1N9zFSGTYaTL4mSFKfDBtJUnWGjQbTgF4TJGnkMWw0mHxNkKQ+GTYaNJm5Hdj5mqBHgaW+Jkj7iohYDHwPeHtE9EbEvKZ7Gkl8g4AkqTrvbCRJ1Rk2kqTqDBtJUnWGjSSpOsNGklSdYSM1ICJ+tpvjk/f07cQRcWNEnPPaOpPqMGwkSdUZNlKDIuLAiLgrIr4fEQ9HRPtbskdHxM0R8WhE3BIRbyjnTI+Ib0fEAxGxIiKOaKh9acAMG6lZW4EPZOZxwHuAv46IKMfeDnw5M38T2AJ8LCLGAF8AzsnM6cBC4KoG+pb2yOimG5BGuAA+ExGnAC/T+kqGw8uxdZn5L2X7fwH/EVgOHAusLJk0CnhqSDuW9oJhIzXrj4AuYHpmbouIJ4Fx5diu75JKWuG0OjPfNXQtSq+dj9GkZr0R2FCC5j3Ar7cdOzIidobKHwLfBR4DunbWI2JMRBwzpB1Le8GwkZp1MzAjIh4Gzgf+b9uxx4AFEfEocChwXfm67XOAz0XED4EHgZOHuGdpj/nWZ0lSdd7ZSJKqM2wkSdUZNpKk6gwbSVJ1ho0kqTrDRpJUnWEjSarOsJEkVff/AX6LfBsTxw/CAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.countplot(x='label',hue='label',data=df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.sample(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140109</th>\n",
       "      <td>I hate how they charge for ambulatory transpor...</td>\n",
       "      <td>I'm going to start moonlighting with my minivan.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545363</th>\n",
       "      <td>I'm just saying that in general and not talkin...</td>\n",
       "      <td>I don't know about you but I certainly would n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660388</th>\n",
       "      <td>What if I told you that many progressive mille...</td>\n",
       "      <td>Yes the FCC decision (one example) really show...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224357</th>\n",
       "      <td>While we are unsure of the attackers motives, ...</td>\n",
       "      <td>Imperialism</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559023</th>\n",
       "      <td>That's kind of the problem. (And why I'm posti...</td>\n",
       "      <td>I mean this sincerely when I say, you're one n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           parent_comment  \\\n",
       "140109  I hate how they charge for ambulatory transpor...   \n",
       "545363  I'm just saying that in general and not talkin...   \n",
       "660388  What if I told you that many progressive mille...   \n",
       "224357  While we are unsure of the attackers motives, ...   \n",
       "559023  That's kind of the problem. (And why I'm posti...   \n",
       "\n",
       "                                                  comment  label  \n",
       "140109   I'm going to start moonlighting with my minivan.      0  \n",
       "545363  I don't know about you but I certainly would n...      0  \n",
       "660388  Yes the FCC decision (one example) really show...      1  \n",
       "224357                                        Imperialism      1  \n",
       "559023  I mean this sincerely when I say, you're one n...      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    tokens_wo_stopwords = []\n",
    "    for i in range(0,len(tokens)):\n",
    "        if tokens[i].lower() not in stop_words:\n",
    "            tokens_wo_stopwords.append(tokens[i].lower())\n",
    "    return tokens_wo_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(token):\n",
    "    pos_tag = nltk.pos_tag([token])[0][1]\n",
    "    if pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(0,len(tokens)):\n",
    "        tokens[i] = lemmatizer.lemmatize(tokens[i],pos=str(get_pos_tag(tokens[i])))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dictionary(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in dictionary:\n",
    "            dictionary.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dictionary():\n",
    "    with open('data/processed/dictionary.txt','w') as file:\n",
    "        file.writelines(\"%s\\n\" % word for word in dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dictionary():\n",
    "    with open('data/processed/dictionary.txt','r') as file:\n",
    "        temp = file.read().splitlines()\n",
    "        for i in range(0,len(temp)):\n",
    "            dictionary.append(temp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(sess,path):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(sess,path):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    processed_sentence = re.sub(r'[^a-zA-Z]', ' ', sentence)\n",
    "    tokens_comment = word_tokenize(processed_sentence)\n",
    "    tokens_comment = remove_stopwords(tokens_comment)\n",
    "    tokens_comment = lemmatize(tokens_comment)\n",
    "    return tokens_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(dataset):\n",
    "    for index,row in dataset.iterrows():\n",
    "        tokens_comment = preprocess(str(row['parent_comment']) + \" \" + str(row['comment']))\n",
    "        add_to_dictionary(tokens_comment)\n",
    "    save_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_dictionary():\n",
    "    if not os.path.isfile('data/processed/dictionary.txt'):\n",
    "        starttime = time.time()\n",
    "        create_dictionary(df_new)\n",
    "        endtime = time.time()\n",
    "        print(\"Time to create dictionary\")\n",
    "        print(endtime - starttime)\n",
    "    else:   \n",
    "        read_dictionary()\n",
    "    print(\"Length of dictionary:- \")\n",
    "    print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_embeddings_dict():\n",
    "    starttime = time.time()\n",
    "    with open('data/processed/glove.6B.300d.txt','r') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            word_embedding = np.asarray(values[1:])\n",
    "            embeddings[word] = word_embedding\n",
    "    endtime = time.time()\n",
    "    print(\"Time taken to load embeddings:- \")\n",
    "    print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tokens(tokens,max_length):\n",
    "    zeros = np.zeros(len(tokens[0]))\n",
    "    while len(tokens) < max_length:\n",
    "        tokens = np.vstack([tokens,zeros])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(X):\n",
    "    max_length = 0\n",
    "    index = 0\n",
    "    for i in range(0,len(X.index)):\n",
    "        if(X[i:i+1][X.index[i]] is not nan):\n",
    "            preprocessed_tokens = preprocess(X[i:i+1][X.index[i]])\n",
    "            if(len(preprocessed_tokens) < 30):\n",
    "                if max_length < len(preprocessed_tokens):\n",
    "                    max_length = len(preprocessed_tokens)\n",
    "                    index = i\n",
    "    print(index)\n",
    "    print(preprocess(X[index:index+1][X.index[index]]))\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup(x,embedding_dim=300):\n",
    "    if(len(embeddings) == 0):\n",
    "        populate_embeddings_dict()\n",
    "    embedding = []\n",
    "    for i in range(0,len(x)):\n",
    "        if(x[i] in embeddings):\n",
    "            embedding.append(embeddings[x[i]])\n",
    "        else:\n",
    "            zero_arr = np.zeros(embedding_dim).tolist()\n",
    "            embedding.append(zero_arr)\n",
    "    return np.array(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elmo_embeddings(sess,tokens_input,tokens_length):\n",
    "    embeddings = elmo(inputs={\"tokens\": tokens_input,\"sequence_len\": tokens_length},signature='tokens',as_dict=True)[\"elmo\"]\n",
    "    return sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deep_contextualized_embeddings(X,y,max_length):\n",
    "    deep_contextualized_embeddings = []\n",
    "    sequence_lengths = []\n",
    "    elmo_tokens = []\n",
    "    elmo_tokens_length = []\n",
    "    elmo_embeddings_list = []\n",
    "    y_pred = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        starttime = time.time()\n",
    "        for i in range(0,len(X.index)):\n",
    "            if(X[i:i+1][X.index[i]] is not nan):\n",
    "                preprocessed_tokens = preprocess(X[i:i+1][X.index[i]])\n",
    "                if(len(preprocessed_tokens) < 30):\n",
    "                    sequence_lengths.append(len(preprocessed_tokens))\n",
    "                    y_pred.append(y[i:i+1][y.index[i]])\n",
    "                    for j in range(len(preprocessed_tokens),max_length):\n",
    "                        preprocessed_tokens.append(\"<PAD>\")\n",
    "                    #word_embedding = embedding_lookup(preprocessed_tokens)\n",
    "                    #word_embedding = np.array(pad_tokens(word_embedding,max_length))\n",
    "                    elmo_tokens.append(preprocessed_tokens)\n",
    "                    elmo_tokens_length.append(len(preprocessed_tokens))\n",
    "                    #deep_contextualized_embeddings.append(np.hstack([word_embedding,elmo_embedding]))\n",
    "                    if (i + 1) % 1000 == 0:\n",
    "                        elmo_embedding = get_elmo_embeddings(sess,np.array(elmo_tokens),np.array(elmo_tokens_length))\n",
    "                        for j in range(0,len(elmo_embedding)):\n",
    "                            deep_contextualized_embeddings.append(np.array(pad_tokens(elmo_embedding[j],max_length)))\n",
    "                        temp_arr = np.array(deep_contextualized_embeddings)\n",
    "                        print(temp_arr.shape)\n",
    "                        elmo_tokens.clear()\n",
    "                        elmo_tokens_length.clear()\n",
    "        endtime = time.time()\n",
    "        print(\"Total time to generate embeddings:- \")\n",
    "        print(endtime - starttime)\n",
    "    return np.array(deep_contextualized_embeddings),np.array(y_pred),np.array(sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_state():\n",
    "    np.save('data/trained_models/generated_embeddings_train.npy',deep_contextualized_embeddings_train)\n",
    "    np.save('data/trained_models/generated_embeddings_test.npy',deep_contextualized_embeddings_test)\n",
    "    np.save('data/trained_models/x_train.npy',X_train)\n",
    "    np.save('data/trained_models/x_test.npy',X_test)\n",
    "    np.save('data/trained_models/y_train.npy',y_train)\n",
    "    np.save('data/trained_models/y_test.npy',y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Remove nan here\n",
    "X = df_new['comment']\n",
    "y = df_new['label']\n",
    "X.reset_index()\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([370296, 201344,  16295, 770592, 995755, 842241, 801848, 342953,\n",
       "            397813,  95426,\n",
       "            ...\n",
       "            650819, 630157, 412251, 609598, 554755, 983740, 413346, 329990,\n",
       "            806189, 192020],\n",
       "           dtype='int64', length=16000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 95498, 367881, 655283, 460760, 913292, 815110, 306456, 823363,\n",
       "            766784, 205574,\n",
       "            ...\n",
       "            613376, 974214, 978936, 428790, 255292, 441333, 788882, 492285,\n",
       "            824284, 773190],\n",
       "           dtype='int64', length=4000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_parent = df_new['parent_comment']\n",
    "X_train_parent,X_test_parent,_,_ = train_test_split(df_new_parent,y,test_size=0.2,random_state=222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_parent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([370296, 201344,  16295, 770592, 995755, 842241, 801848, 342953,\n",
       "            397813,  95426,\n",
       "            ...\n",
       "            650819, 630157, 412251, 609598, 554755, 983740, 413346, 329990,\n",
       "            806189, 192020],\n",
       "           dtype='int64', length=16000)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_parent.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_parent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 95498, 367881, 655283, 460760, 913292, 815110, 306456, 823363,\n",
       "            766784, 205574,\n",
       "            ...\n",
       "            613376, 974214, 978936, 428790, 255292, 441333, 788882, 492285,\n",
       "            824284, 773190],\n",
       "           dtype='int64', length=4000)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_parent.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9111\n",
      "['usually', 'beautiful', 'something', 'societally', 'appeal', 'rally', 'around', 'people', 'get', 'judge', 'without', 'consequence', 'enjoy', 'jealous', 'talent', 'beauty', 'fame', 'etc', 'watch', 'every', 'move', 'necessarily', 'think', 'celebrity', 'worship', 'make', 'sense', 'reason', 'popular']\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0414 15:42:10.929646 4704441792 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "deep_contextualized_embeddings_train,y_pred_train,sequence_lengths_train = get_deep_contextualized_embeddings(X_train,y_train,get_max_length(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_contextualized_embeddings_parent_train,_,sequence_lengths_parent_train = get_deep_contextualized_embeddings(X_train_parent,y_train,get_max_length(X_train_parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_contextualized_embeddings_test,y_pred_test,sequence_lengths_test = get_deep_contextualized_embeddings(X_test,y_test,get_max_length(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_contextualized_embeddings_parent_test,_,sequence_lengths_parent_test = get_deep_contextualized_embeddings(X_test_parent,y_test,get_max_length(X_test_parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_contextualized_embeddings_train = np.array(deep_contextualized_embeddings_train)\n",
    "deep_contextualized_embeddings_parent_train = np.array(deep_contextualized_embeddings_parent_train)\n",
    "deep_contextualized_embeddings_test = np.array(deep_contextualized_embeddings_test)\n",
    "deep_contextualized_embeddings_parent_test = np.array(deep_contextualized_embeddings_parent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_contextualized_embeddings_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_contextualized_embeddings_parent_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_contextualized_embeddings_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_contextualized_embeddings_parent_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM():\n",
    "    \n",
    "    def __init__(self,num_classes,elmo_embed_size,embed_size,batch_size,epochs,init_learning_rate):\n",
    "        self.X = tf.placeholder(shape=[None,None,embed_size + elmo_embed_size],dtype=tf.float32,name='X')\n",
    "        self.y = tf.placeholder(shape=[None],dtype=tf.int64,name='y')\n",
    "        self.sequence_lengths = tf.placeholder(shape=[None],dtype=tf.int32,name='sequence_lengths')\n",
    "        self.num_classes = num_classes\n",
    "        self.elmo_embed_size = elmo_embed_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = elmo_embed_size + embed_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.init_learning_rate = init_learning_rate\n",
    "        self.model()\n",
    "    \n",
    "    def model(self):\n",
    "        cell_fw = tf.contrib.rnn.LSTMCell(self.hidden_size,forget_bias=1.0,state_is_tuple=True,reuse=tf.get_variable_scope().reuse)\n",
    "        cell_bw = tf.contrib.rnn.LSTMCell(self.hidden_size,forget_bias=1.0,state_is_tuple=True,reuse=tf.get_variable_scope().reuse)\n",
    "        with tf.variable_scope('Bi-Directional-LSTM',reuse=tf.AUTO_REUSE):\n",
    "            output_vals,output_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw = cell_fw,\n",
    "            cell_bw = cell_bw,\n",
    "            inputs = self.X,\n",
    "            sequence_length = self.sequence_lengths,\n",
    "            dtype = tf.float32)\n",
    "        self.final_state = tf.concat([output_states[0].c,output_states[1].c],axis=1)\n",
    "        with tf.variable_scope('Softmax',reuse=tf.AUTO_REUSE):\n",
    "            self.softmax_w = tf.get_variable('softmax_w',shape=[2 * self.hidden_size,self.num_classes],initializer=tf.truncated_normal_initializer(),dtype=tf.float32)\n",
    "            self.softmax_b = tf.get_variable('softmax_b',shape=[self.num_classes],initializer=tf.constant_initializer(0.0),dtype=tf.float32)\n",
    "        self.logits = tf.matmul(self.final_state,self.softmax_w) + self.softmax_b\n",
    "        self.predictions = tf.argmax(tf.nn.softmax(self.logits),1,name='predictions')\n",
    "        self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y,logits=self.logits)\n",
    "        self.cost = tf.reduce_mean(self.cross_entropy)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.predictions,self.y),tf.float32),name='accuracy')\n",
    "    \n",
    "    def train(self,X_train,y_train,sequence_lengths_train,path):\n",
    "        self.global_step = tf.Variable(0,name='global_step',trainable=False)\n",
    "        self.learning_rate = tf.train.exponential_decay(self.init_learning_rate,self.global_step,8,1,staircase=True)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        grads = optimizer.compute_gradients(self.cost)\n",
    "        self.train_step = optimizer.apply_gradients(grads,global_step=self.global_step)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for j in range(0,self.epochs):\n",
    "                starttime = time.time()\n",
    "                epoch_cost = 0\n",
    "                for i in range(0,math.ceil(X_train.shape[0]/self.batch_size)):\n",
    "                    X_train_batch = X_train[i * self.batch_size : min((i + 1) * self.batch_size,X_train.shape[0])]\n",
    "                    y_train_batch = y_train[i * self.batch_size : min((i + 1) * self.batch_size,len(y_train))]\n",
    "                    sequence_lengths_batch = sequence_lengths_train[i * self.batch_size : min((i + 1) * self.batch_size,len(sequence_lengths_train))]\n",
    "                    fetches = {\n",
    "                        'cross_entropy': self.cross_entropy,\n",
    "                        'cost': self.cost,\n",
    "                        'train_step': self.train_step,\n",
    "                        'learning_rate': self.learning_rate,\n",
    "                        'global_step': self.global_step\n",
    "                    }\n",
    "                    feed_dict = {\n",
    "                        self.X : X_train_batch,\n",
    "                        self.y : y_train_batch,\n",
    "                        self.sequence_lengths : sequence_lengths_batch\n",
    "                    }\n",
    "                    resp = sess.run(fetches,feed_dict)\n",
    "                    print('Learning rate:- ')\n",
    "                    print(resp['learning_rate'])\n",
    "                    print('Global Step:- ')\n",
    "                    print(resp['global_step'])\n",
    "                    epoch_cost += resp['cost']\n",
    "                endtime = time.time()\n",
    "                print('Time to train epoch ' + str(j) + ':-')\n",
    "                print(endtime - starttime)\n",
    "                print('Epoch ' + str(j) + \" cost :-\")\n",
    "                print(epoch_cost)\n",
    "            save_model(sess,path)\n",
    "            \n",
    "    def test(self,X_test,y_test,sequence_lengths_test,path):\n",
    "        with tf.Session() as sess:\n",
    "            starttime = time.time()\n",
    "            load_model(sess,path)\n",
    "            fetches = {\n",
    "                'accuracy': self.accuracy,\n",
    "                'predictions': self.predictions\n",
    "            }\n",
    "            feed_dict = {\n",
    "                self.X : X_test,\n",
    "                self.y : y_test,\n",
    "                self.sequence_lengths: sequence_lengths_test\n",
    "            }\n",
    "            resp = sess.run(fetches,feed_dict)\n",
    "            endtime = time.time()\n",
    "            print('Time to test model:- ')\n",
    "            print(endtime - starttime)\n",
    "            print('Model accuracy:- ')\n",
    "            print(resp['accuracy'])\n",
    "            print('Model predictions:- ')\n",
    "            print(resp['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters for the model\n",
    "#Hyperparameter tuning required\n",
    "num_classes = 2\n",
    "word_embedding_size = 0 #For now as we are currently not using Glove\n",
    "elmo_embedding_size = 1024\n",
    "batch_size = 1000\n",
    "epochs = 10 #To be increased as the size of the dataset increases(current size being considered:- 10000 data points (8000 - Train,2000 - Test))\n",
    "init_learning_rate = 0.01 #To be changed to exponentially decreasing value based on epochs passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Currenlty not using concatenation of Glove with ELMO\n",
    "tf.reset_default_graph()\n",
    "lstm = LSTM(num_classes,word_embedding_size,elmo_embedding_size,batch_size,epochs,init_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm.train(deep_contextualized_embeddings_train,y_pred_train,sequence_lengths_train,'data/trained_models/elmo_bi_directional_lstm.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.test(deep_contextualized_embeddings_test,y_pred_test,sequence_lengths_test,'data/trained_models/elmo_bi_directional_lstm.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

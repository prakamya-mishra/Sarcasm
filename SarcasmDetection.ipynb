{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashvatkedia/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0413 11:16:07.354928 4432184768 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset/train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[['parent_comment','comment','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a208a65c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.countplot(x='label',hue='label',data=df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108139</th>\n",
       "      <td>Bans For Climbing: They're More Important Than...</td>\n",
       "      <td>Did you forget that graves exists?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625770</th>\n",
       "      <td>How is Quin doubtful? He practiced all week an...</td>\n",
       "      <td>Caldwell psychological mastermind</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535022</th>\n",
       "      <td>Serious question what is with the new trend to...</td>\n",
       "      <td>Well clearly they are nothing more than sacks ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424938</th>\n",
       "      <td>This is pretty spot on. They are a party for p...</td>\n",
       "      <td>Spot on.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641845</th>\n",
       "      <td>Don't underestimate the almighty Koch brothers!</td>\n",
       "      <td>All hail the mighty Koch brothers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           parent_comment  \\\n",
       "108139  Bans For Climbing: They're More Important Than...   \n",
       "625770  How is Quin doubtful? He practiced all week an...   \n",
       "535022  Serious question what is with the new trend to...   \n",
       "424938  This is pretty spot on. They are a party for p...   \n",
       "641845    Don't underestimate the almighty Koch brothers!   \n",
       "\n",
       "                                                  comment  label  \n",
       "108139                 Did you forget that graves exists?      0  \n",
       "625770                  Caldwell psychological mastermind      1  \n",
       "535022  Well clearly they are nothing more than sacks ...      1  \n",
       "424938                                           Spot on.      0  \n",
       "641845                  All hail the mighty Koch brothers      1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    tokens_wo_stopwords = []\n",
    "    for i in range(0,len(tokens)):\n",
    "        if tokens[i].lower() not in stop_words:\n",
    "            tokens_wo_stopwords.append(tokens[i].lower())\n",
    "    return tokens_wo_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(token):\n",
    "    pos_tag = nltk.pos_tag([token])[0][1]\n",
    "    if pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(0,len(tokens)):\n",
    "        tokens[i] = lemmatizer.lemmatize(tokens[i],pos=str(get_pos_tag(tokens[i])))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dictionary(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in dictionary:\n",
    "            dictionary.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dictionary():\n",
    "    with open('data/processed/dictionary.txt','w') as file:\n",
    "        file.writelines(\"%s\\n\" % word for word in dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dictionary():\n",
    "    with open('data/processed/dictionary.txt','r') as file:\n",
    "        temp = file.read().splitlines()\n",
    "        for i in range(0,len(temp)):\n",
    "            dictionary.append(temp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    processed_sentence = re.sub(r'[^a-zA-Z]', ' ', sentence)\n",
    "    tokens_comment = word_tokenize(processed_sentence)\n",
    "    tokens_comment = remove_stopwords(tokens_comment)\n",
    "    tokens_comment = lemmatize(tokens_comment)\n",
    "    return tokens_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(dataset):\n",
    "    for index,row in dataset.iterrows():\n",
    "        tokens_comment = preprocess(str(row['parent_comment']) + \" \" + str(row['comment']))\n",
    "        add_to_dictionary(tokens_comment)\n",
    "    save_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_embeddings_dict():\n",
    "    starttime = time.time()\n",
    "    with open('data/processed/glove.6B.300d.txt','r') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            word_embedding = np.asarray(values[1:])\n",
    "            embeddings[word] = word_embedding\n",
    "    endtime = time.time()\n",
    "    print(\"Time taken to load embeddings:- \")\n",
    "    print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup(x,embedding_dim=300):\n",
    "    if(len(embeddings) == 0):\n",
    "        populate_embeddings_dict()\n",
    "    embedding = []\n",
    "    for i in range(0,len(x)):\n",
    "        if(x[i] in embeddings):\n",
    "            embedding.append(embeddings[x[i]])\n",
    "        else:\n",
    "            zero_arr = np.zeros(embedding_dim).tolist()\n",
    "            embedding.append(zero_arr)\n",
    "    return np.array(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/processed/dictionary.txt'):\n",
    "    starttime = time.time()\n",
    "    create_dictionary(df_new)\n",
    "    endtime = time.time()\n",
    "    print(\"Time to create dictionary\")\n",
    "    print(endtime - starttime)\n",
    "else:   \n",
    "    read_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67028"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forget', 'graf', 'exists']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(df_new['comment'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\",trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elmo_embeddings(sess,tokens_input,tokens_length):\n",
    "    embeddings = elmo(inputs={\"tokens\": tokens_input,\"sequence_len\": tokens_length},signature='tokens',as_dict=True)[\"elmo\"]\n",
    "    return sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 11:17:55.599807 4432184768 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "tokens = preprocess(df_new['comment'].iloc[0])\n",
    "tokens_input = [tokens]\n",
    "tokens_length = [len(tokens)]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    elmo_embeddings = get_elmo_embeddings(sess,tokens_input,tokens_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 1024)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMO embeddings for 'cool'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.07532147, -0.5643488 ,  0.23921366, ..., -0.37776455,\n",
       "        0.25982827,  0.09048565], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"ELMO embeddings for 'cool'\")\n",
    "elmo_embeddings[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_elmo_embedding():\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        for index,rows in df_new.iterrows():\n",
    "            preprocessed_tokens = preprocess(rows['parent_comment'] + \" \" + rows['comment'])\n",
    "            embedding_tensor = elmo(preprocessed_tokens)\n",
    "            embeddings = sess.run(embedding_tensor)\n",
    "            #Rest of the network here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM():\n",
    "    \n",
    "    def __init__(self,num_classes,elmo_embed_size,embed_size,batch_size,epochs,learning_rate):\n",
    "        self.X = tf.placeholder(shape=[None,None,embed_size + elmo_embed_size],dtype=tf.float32,name='X')\n",
    "        self.y = tf.placeholder(shape=[None],dtype=tf.int64,name='y')\n",
    "        self.sequence_lengths = tf.placeholder(shape=[None],dtype=tf.int32,name='sequence_lengths')\n",
    "        self.num_classes = num_classes\n",
    "        self.elmo_embed_size = elmo_embed_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = elmo_embed_size + embed_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model()\n",
    "    \n",
    "    def model(self):\n",
    "        cell_fw = tf.contrib.rnn.LSTMCell(self.hidden_size,forget_bias=1.0,state_is_tuple=True,reuse=tf.get_variable_scope().reuse)\n",
    "        cell_bw = tf.contrib.rnn.LSTMCell(self.hidden_size,forget_bias=1.0,state_is_tuple=True,reuse=tf.get_variable_scope().reuse)\n",
    "        with tf.variable_scope('Bi-Directional-LSTM',reuse=tf.AUTO_REUSE):\n",
    "            output_vals,output_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw = cell_fw,\n",
    "            cell_bw = cell_bw,\n",
    "            inputs = self.X,\n",
    "            sequence_length = self.sequence_lengths,\n",
    "            dtype = tf.float32)\n",
    "        self.final_state = tf.concat([output_states[0].c,output_states[1].c],axis=1)\n",
    "        with tf.variable_scope('Softmax',reuse=tf.AUTO_REUSE):\n",
    "            self.softmax_w = tf.get_variable('softmax_w',shape=[2 * self.hidden_size,self.num_classes],initializer=tf.truncated_normal_initializer(),dtype=tf.float32)\n",
    "            self.softmax_b = tf.get_variable('softmax_b',shape=[self.num_classes],initializer=tf.constant_initializer(0.0),dtype=tf.float32)\n",
    "        self.logits = tf.matmul(self.final_state,self.softmax_w) + self.softmax_b\n",
    "        self.predictions = tf.argmax(tf.nn.softmax(self.logits),1,name='predictions')\n",
    "        self.cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y,logits=self.logits))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.predictions,self.y),tf.float32),name='accuracy')\n",
    "    \n",
    "    def train(self,X_train,X_test,y_train,y_test,sequence_lengths_train,sequence_lengths_test):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        train_step = optimizer.minimize(self.cost)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for j in range(0,self.epochs):\n",
    "                for i in range(0,X_train.shape[0],self.batch_size):\n",
    "                    X_train_batch = X_train[i * self.batch_size : min(i * (self.batch_size + 1) - 1,X_train.shape[0] - (i * self.batch_size) + 1)]\n",
    "                    y_train_batch = y_train[i * self.batch_size : min(i * (self.batch_size + 1) - 1,len(y_train) - (i * self.batch_size) + 1)]\n",
    "                    sequence_lengths_batch = sequence_lengths_train[i * self.batch_size : min(i * (self.batch_size + 1) - 1,len(sequence_lengths_train) - (i * self.batch_size) + 1)]\n",
    "                    fetches = {\n",
    "                        'cost': self.cost,\n",
    "                        'accuracy': self.accuracy,\n",
    "                        'predictions': self.predictions\n",
    "                    }\n",
    "                    feed_dict = {\n",
    "                        self.X : X_train_batch,\n",
    "                        self.y : y_train_batch,\n",
    "                        self.sequence_lengths : sequence_lengths_batch\n",
    "                    }\n",
    "                    resp = sess.run(fetches,feed_dict)\n",
    "                print(resp['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning required\n",
    "#Currenlty not using concatenation of Glove with ELMO\n",
    "tf.reset_default_graph()\n",
    "lstm = LSTM(2,1024,0,4000,5,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108139                   Did you forget that graves exists?\n",
      "625770                    Caldwell psychological mastermind\n",
      "535022    Well clearly they are nothing more than sacks ...\n",
      "424938                                             Spot on.\n",
      "641845                    All hail the mighty Koch brothers\n",
      "Name: comment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Remove nan here\n",
    "X = df_new['comment']\n",
    "y = df_new['label']\n",
    "X.reset_index()\n",
    "print(X.head())\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([116108, 418845,  98973, 268148, 348009, 946242,  79757, 936148,\n",
       "            262936, 429464,\n",
       "            ...\n",
       "            665473, 497289, 817244, 994677, 891568, 733907, 557936, 919988,\n",
       "            679327, 277196],\n",
       "           dtype='int64', length=4000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([602869, 331291, 512440, 737814, 894030, 516838, 384983, 761767,\n",
       "            958165, 503167,\n",
       "            ...\n",
       "            722245, 291146, 334746, 595844, 934192, 276631, 244861, 535211,\n",
       "            653442, 274847],\n",
       "           dtype='int64', length=1000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tokens(tokens,max_length):\n",
    "    zeros = np.zeros(len(tokens[0]))\n",
    "    while len(tokens) < max_length:\n",
    "        tokens = np.vstack([tokens,zeros])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(X):\n",
    "    max_length = 0\n",
    "    index = 0\n",
    "    for i in range(0,len(X.index)):\n",
    "        if(X[i:i+1][X.index[i]] is not nan):\n",
    "            preprocessed_tokens = preprocess(X[i:i+1][X.index[i]])\n",
    "            if(len(preprocessed_tokens) < 30):\n",
    "                if max_length < len(preprocessed_tokens):\n",
    "                    max_length = len(preprocessed_tokens)\n",
    "                    index = i\n",
    "    print(index)\n",
    "    print(preprocess(X[index:index+1][X.index[index]]))\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1551\n",
      "['would', 'thought', 'drug', 'make', 'people', 'lounge', 'around', 'sofa', 'day', 'nothing', 'stenuous', 'reach', 'remote', 'another', 'taco', 'play', 'video', 'game', 'would', 'help', 'people', 'heal', 'need', 'lounge', 'around', 'relax', 'day']\n",
      "Time to calculate max length:- \n",
      "6.697695970535278\n",
      "Max length:- \n",
      "27\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "max_length = get_max_length(X_train)\n",
    "endtime = time.time()\n",
    "print(\"Time to calculate max length:- \")\n",
    "print(endtime - starttime)\n",
    "print(\"Max length:- \")\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_deep_contextualized_embeddings(X,y,max_length):\n",
    "    deep_contextualized_embeddings = []\n",
    "    sequence_lengths = []\n",
    "    elmo_tokens = []\n",
    "    elmo_tokens_length = []\n",
    "    elmo_embeddings_list = []\n",
    "    y_pred = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        starttime = time.time()\n",
    "        for i in range(0,len(X.index)):\n",
    "            if(X[i:i+1][X.index[i]] is not nan):\n",
    "                preprocessed_tokens = preprocess(X[i:i+1][X.index[i]])\n",
    "                if(len(preprocessed_tokens) < 30):\n",
    "                    sequence_lengths.append(len(preprocessed_tokens))\n",
    "                    y_pred.append(y[i:i+1][y.index[i]])\n",
    "                    for j in range(len(preprocessed_tokens),max_length):\n",
    "                        preprocessed_tokens.append(\"<PAD>\")\n",
    "                    #word_embedding = embedding_lookup(preprocessed_tokens)\n",
    "                    #word_embedding = np.array(pad_tokens(word_embedding,max_length))\n",
    "                    elmo_tokens.append(preprocessed_tokens)\n",
    "                    elmo_tokens_length.append(len(preprocessed_tokens))\n",
    "                    #deep_contextualized_embeddings.append(np.hstack([word_embedding,elmo_embedding]))\n",
    "                    if (i + 1) % 1000 == 0:\n",
    "                        elmo_embedding = get_elmo_embeddings(sess,np.array(elmo_tokens),np.array(elmo_tokens_length))\n",
    "                        for j in range(0,len(elmo_embedding)):\n",
    "                            deep_contextualized_embeddings.append(np.array(pad_tokens(elmo_embedding[j],max_length)))\n",
    "                        temp_arr = np.array(deep_contextualized_embeddings)\n",
    "                        print(temp_arr.shape)\n",
    "                        elmo_tokens.clear()\n",
    "                        elmo_tokens_length.clear()\n",
    "        endtime = time.time()\n",
    "        print(\"Total time to generate embeddings:- \")\n",
    "        print(endtime - starttime)\n",
    "    return np.array(deep_contextualized_embeddings),np.array(y_pred),np.array(sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1551\n",
      "['would', 'thought', 'drug', 'make', 'people', 'lounge', 'around', 'sofa', 'day', 'nothing', 'stenuous', 'reach', 'remote', 'another', 'taco', 'play', 'video', 'game', 'would', 'help', 'people', 'heal', 'need', 'lounge', 'around', 'relax', 'day']\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 12:11:00.342935 4432184768 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(997, 27, 1024)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 12:18:02.137594 4432184768 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1995, 27, 1024)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 12:21:54.396167 4432184768 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2993, 27, 1024)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 12:26:55.758517 4432184768 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3991, 27, 1024)\n",
      "Total time to generate embeddings:- \n",
      "1257.2532160282135\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-3e6347be95cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdeep_contextualized_embeddings_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_lengths_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_deep_contextualized_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mget_max_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdeep_contextualized_embeddings_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_lengths_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_deep_contextualized_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mget_max_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "deep_contextualized_embeddings_train,y_pred_train,sequence_lengths_train = get_deep_contextualized_embeddings(X_train,y_train,get_max_length(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633\n",
      "['brother', 'pen', 'tester', 'white', 'hat', 'ethical', 'hacker', 'whatever', 'fuck', 'want', 'call', 'nickle', 'every', 'linux', 'system', 'see', 'chickenshit', 'security', 'well', 'let', 'say', 'could', 'reddit', 'living']\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 12:36:11.893086 4432184768 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999, 24, 1024)\n",
      "Total time to generate embeddings:- \n",
      "142.5271852016449\n"
     ]
    }
   ],
   "source": [
    "deep_contextualized_embeddings_test,y_pred_test,sequence_lengths_test = get_deep_contextualized_embeddings(X_test,t_test,get_max_length(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_contextualized_embeddings_train = np.array(deep_contextualized_embeddings_train)\n",
    "deep_contextualized_embeddings_test = np.array(deep_contextualized_embeddings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3991, 27, 1024)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_contextualized_embeddings_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 24, 1024)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_contextualized_embeddings_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.train(deep_contextualized_embeddings_train,deep_contextualized_embeddings_test,y_pred_train,y_pred_test,sequence_lengths_train,sequence_lengths_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

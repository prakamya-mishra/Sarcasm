{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset/train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[['parent_comment','comment','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a5d0c58d0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.countplot(x='label',hue='label',data=df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>782258</th>\n",
       "      <td>Dear Libertarians I wish that you were all dea...</td>\n",
       "      <td>Cool story, bro.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617258</th>\n",
       "      <td>I am a person who is unhappy with Starbound. I...</td>\n",
       "      <td>That and outside of the main quest there is ab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646561</th>\n",
       "      <td>So I calculated that on a website im 5' 11\" an...</td>\n",
       "      <td>Not a lot more but a small surplus of a few hu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157433</th>\n",
       "      <td>Socialists who argue that one need only look a...</td>\n",
       "      <td>No, no, no, we just didn't try hard enough!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298149</th>\n",
       "      <td>It's written in stone. Don't blame \"people lik...</td>\n",
       "      <td>Yeah, well, we are working on carving a new me...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           parent_comment  \\\n",
       "782258  Dear Libertarians I wish that you were all dea...   \n",
       "617258  I am a person who is unhappy with Starbound. I...   \n",
       "646561  So I calculated that on a website im 5' 11\" an...   \n",
       "157433  Socialists who argue that one need only look a...   \n",
       "298149  It's written in stone. Don't blame \"people lik...   \n",
       "\n",
       "                                                  comment  label  \n",
       "782258                                   Cool story, bro.      0  \n",
       "617258  That and outside of the main quest there is ab...      0  \n",
       "646561  Not a lot more but a small surplus of a few hu...      0  \n",
       "157433        No, no, no, we just didn't try hard enough!      1  \n",
       "298149  Yeah, well, we are working on carving a new me...      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    tokens_wo_stopwords = []\n",
    "    for i in range(0,len(tokens)):\n",
    "        if tokens[i].lower() not in stop_words:\n",
    "            tokens_wo_stopwords.append(tokens[i].lower())\n",
    "    return tokens_wo_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(token):\n",
    "    pos_tag = nltk.pos_tag([token])[0][1]\n",
    "    if pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(0,len(tokens)):\n",
    "        tokens[i] = lemmatizer.lemmatize(tokens[i],pos=str(get_pos_tag(tokens[i])))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dictionary(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in dictionary:\n",
    "            dictionary.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dictionary():\n",
    "    with open('data/processed/dictionary.txt','w') as file:\n",
    "        file.writelines(\"%s\\n\" % word for word in dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dictionary():\n",
    "    with open('data/processed/dictionary.txt','r') as file:\n",
    "        temp = file.read().splitlines()\n",
    "        for i in range(0,len(temp)):\n",
    "            dictionary.append(temp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    processed_sentence = re.sub(r'[^a-zA-Z]', ' ', sentence)\n",
    "    tokens_comment = word_tokenize(processed_sentence)\n",
    "    tokens_comment = remove_stopwords(tokens_comment)\n",
    "    tokens_comment = lemmatize(tokens_comment)\n",
    "    return tokens_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(dataset):\n",
    "    for index,row in dataset.iterrows():\n",
    "        tokens_comment = preprocess(str(row['parent_comment']) + \" \" + str(row['comment']))\n",
    "        add_to_dictionary(tokens_comment)\n",
    "    save_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_embeddings_dict():\n",
    "    starttime = time.time()\n",
    "    with open('data/processed/glove.6B.300d.txt','r') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            word_embedding = np.asarray(values[1:])\n",
    "            embeddings[word] = word_embedding\n",
    "    endtime = time.time()\n",
    "    print(\"Time taken to load embeddings:- \")\n",
    "    print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup(x,embedding_dim=300):\n",
    "    if(len(embeddings) == 0):\n",
    "        populate_embeddings_dict()\n",
    "    embedding = []\n",
    "    for i in range(0,len(x)):\n",
    "        if(x[i] in embeddings):\n",
    "            embedding.append(embeddings[x[i]])\n",
    "        else:\n",
    "            zero_arr = np.zeros(embedding_dim).tolist()\n",
    "            embedding.append(zero_arr)\n",
    "    embedding = np.array(embedding)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/processed/dictionary.txt'):\n",
    "    starttime = time.time()\n",
    "    create_dictionary(df_new)\n",
    "    endtime = time.time()\n",
    "    print(\"Time to create dictionary\")\n",
    "    print(endtime - starttime)\n",
    "else:   \n",
    "    read_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67028"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load embeddings:- \n",
      "35.096932888031006\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedding_lookup(preprocess(df_new['comment'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cool', 'story', 'bro']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(df_new['comment'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\",trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elmo_embeddings(tokens_input,tokens_length):\n",
    "    embeddings = elmo(inputs={\"tokens\": tokens_input,\"sequence_len\": tokens_length},signature='tokens',as_dict=True)[\"elmo\"]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        return sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = preprocess(df_new['comment'].iloc[0])\n",
    "tokens_input = [tokens]\n",
    "tokens_length = [len(tokens)]\n",
    "elmo_embeddings = get_elmo_embeddings(tokens_input,tokens_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 1024)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMO embeddings for 'cool'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.08699809, -0.67682254,  0.91641057, ..., -0.30401048,\n",
       "        0.30957115,  0.37923324], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"ELMO embeddings for 'cool'\")\n",
    "elmo_embeddings[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_elmo_embedding():\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        for index,rows in df_new.iterrows():\n",
    "            preprocessed_tokens = preprocess(rows['parent_comment'] + \" \" + rows['comment'])\n",
    "            embedding_tensor = elmo(preprocessed_tokens)\n",
    "            embeddings = sess.run(embedding_tensor)\n",
    "            #Rest of the network here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM():\n",
    "    \n",
    "    def __init__(self,num_classes,elmo_emmed_size,embed_size):\n",
    "        self.X = tf.placeholder(shape=[None,None,embed_size + elmo_embed_size])\n",
    "        self.y = tf.placeholder(shape=[None])\n",
    "        self.sequence_lengths = tf.placeholder(shape=[None])\n",
    "        self.num_classes = num_classes\n",
    "        self.elmo_embed_size = elmo_embed_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = elmo_embed_size + embed_size\n",
    "        self.X_train,self.X_test,self.y_train,self.y_test = train_test_split(X,y,test_size=0.2,random_state=222)\n",
    "    \n",
    "    def model(self):\n",
    "        cell_fw = tf.contrib.rnn.LSTMCell(self.hidden_size,forget_bias=1.0,state_as_tuple=True,reuse=tf.get_variable_scope().reuse)\n",
    "        cell_bw = tf.contrib.rnn.LSTMCell(self.hidden_size,forget_bias=1.0,state_as_tuple=True,reuse=tf.get_variable_scope().reuse)\n",
    "        with tf.variable_scope('Bi-Directional-LSTM'):\n",
    "            (output_fw,output_bw), (output_state_fw, output_state_bw) = tf.nn.bidirectional_dynamic_lstm(\n",
    "            cell_fw = cell_fw,\n",
    "            cell_bw = cell_bw,\n",
    "            inputs = self.X,\n",
    "            sequence_lengths = self.sequence_lengths,\n",
    "            dtype = tf.float32)\n",
    "        self.final_state = tf.concat([output_state_fw,output_state_bx],axis=1)\n",
    "        with tf.variable_scope('Softmax'):\n",
    "            self.softmax_w = tf.get_variable('softmax_w',shape=[2 * self.hidden_size,self.num_classes],initializer=tf.truncated_normal_initializer(),dtype=tf.float32)\n",
    "            self.softmax_b = tf.get_variable('softmax_b',shape=[self.num_classes],initializer=tf.constant_initializer(0.0),dtype=tf.float32)\n",
    "        self.logits = tf.matmul(self.final_state,self.softmax_w) + self.softmax_b\n",
    "        self.predictions = tf.argmax(tf.nn.softmax(self.logits),1,name='predictions')\n",
    "        self.cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(self.y,self.logits))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(,tf.float32),name='accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

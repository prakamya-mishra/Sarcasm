{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset/train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[['parent_comment','comment','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>216199</th>\n",
       "      <td>SSSSSNEK</td>\n",
       "      <td>Having lived in Germany for a bit, if I saw Ge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237378</th>\n",
       "      <td>If I'm Lue, Shump just played his final mins t...</td>\n",
       "      <td>for sure this game</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191594</th>\n",
       "      <td>dati pa, ngayon lang ako nag post dahil na lat...</td>\n",
       "      <td>Ingat ka sa sinasabi mo baka may magalit kasi ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884645</th>\n",
       "      <td>Denmark.</td>\n",
       "      <td>Tolerance towards hate groups</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705286</th>\n",
       "      <td>My guess is the only reason they made it a blu...</td>\n",
       "      <td>Yea...i would have been totally pissed if I go...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           parent_comment  \\\n",
       "216199                                           SSSSSNEK   \n",
       "237378  If I'm Lue, Shump just played his final mins t...   \n",
       "191594  dati pa, ngayon lang ako nag post dahil na lat...   \n",
       "884645                                           Denmark.   \n",
       "705286  My guess is the only reason they made it a blu...   \n",
       "\n",
       "                                                  comment  label  \n",
       "216199  Having lived in Germany for a bit, if I saw Ge...      1  \n",
       "237378                                 for sure this game      0  \n",
       "191594  Ingat ka sa sinasabi mo baka may magalit kasi ...      1  \n",
       "884645                      Tolerance towards hate groups      0  \n",
       "705286  Yea...i would have been totally pissed if I go...      1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    tokens_wo_stopwords = []\n",
    "    for i in range(0,len(tokens)):\n",
    "        if tokens[i].lower() not in stop_words:\n",
    "            tokens_wo_stopwords.append(tokens[i].lower())\n",
    "    return tokens_wo_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(token):\n",
    "    pos_tag = nltk.pos_tag([token])[0][1]\n",
    "    if pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(0,len(tokens)):\n",
    "        tokens[i] = lemmatizer.lemmatize(tokens[i],pos=str(get_pos_tag(tokens[i])))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dictionary(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in dictionary:\n",
    "            dictionary.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dictionary():\n",
    "    with open('data/processed/dictionary.txt','w') as file:\n",
    "        file.writelines(\"%s\\n\" % word for word in dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dictionary():\n",
    "    with open('data/processed/dictionary.txt','r') as file:\n",
    "        temp = file.read().splitlines()\n",
    "        for i in range(0,len(temp)):\n",
    "            dictionary.append(temp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    processed_sentence = re.sub(r'[^a-zA-Z]', ' ', sentence)\n",
    "    tokens_comment = word_tokenize(processed_sentence)\n",
    "    tokens_comment = remove_stopwords(tokens_comment)\n",
    "    tokens_comment = lemmatize(tokens_comment)\n",
    "    return tokens_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(dataset):\n",
    "    for index,row in dataset.iterrows():\n",
    "        tokens_comment = preprocess(str(row['parent_comment']) + \" \" + str(row['comment']))\n",
    "        add_to_dictionary(tokens_comment)\n",
    "    save_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_embeddings_dict():\n",
    "    starttime = time.time()\n",
    "    with open('data/processed/glove.6B.300d.txt','r') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            word_embedding = np.asarray(values[1:])\n",
    "            embeddings[word] = word_embedding\n",
    "    endtime = time.time()\n",
    "    print(\"Time taken to load embeddings:- \")\n",
    "    print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup(x,embedding_dim=300):\n",
    "    if(len(embeddings) == 0):\n",
    "        populate_embeddings_dict()\n",
    "    embedding = []\n",
    "    for i in range(0,len(x)):\n",
    "        if(x[i] in embeddings):\n",
    "            embedding.append(embeddings[x[i]])\n",
    "        else:\n",
    "            zero_arr = np.zeros(embedding_dim).tolist()\n",
    "            embedding.append(zero_arr)\n",
    "    embedding = np.array(embedding)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to create dictionary\n",
      "430.11281394958496\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('data/processed/dictionary.txt'):\n",
    "    starttime = time.time()\n",
    "    create_dictionary(df_new)\n",
    "    endtime = time.time()\n",
    "    print(\"Time to create dictionary\")\n",
    "    print(endtime - starttime)\n",
    "else:   \n",
    "    read_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67028"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding_lookup(preprocess(df_new['comment'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['-0.36633', '0.48371', '-0.31369', ..., '0.22848', '0.023664',\n",
       "        '-0.075187'],\n",
       "       ['0.046596', '0.18904', '-0.46218', ..., '-0.54401', '0.72344',\n",
       "        '0.0443'],\n",
       "       ['0.082004', '-0.12691', '-0.03035', ..., '0.23209', '0.19432',\n",
       "        '0.18716'],\n",
       "       ...,\n",
       "       ['-0.14124', '-0.11836', '-0.30782', ..., '-0.19883', '-0.061105',\n",
       "        '0.11568'],\n",
       "       ['-0.24586', '-0.28032', '-0.23196', ..., '0.20135', '0.11195',\n",
       "        '0.099126'],\n",
       "       ['0.34831', '0.13124', '0.088767', ..., '0.055823', '0.30498',\n",
       "        '-0.036958']], dtype='<U11')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['live',\n",
       " 'germany',\n",
       " 'bit',\n",
       " 'saw',\n",
       " 'german',\n",
       " 'color',\n",
       " 'follow',\n",
       " 's',\n",
       " 'get',\n",
       " 'little',\n",
       " 'nervous']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(df_new['comment'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0.31001', '0.046907', '-0.31283', '-0.2605', '-0.3749',\n",
       "       '-0.13487', '-0.2176', '0.59326', '-0.30687', '-1.4446',\n",
       "       '-0.26943', '0.45318', '-0.14138', '0.030587', '0.13575',\n",
       "       '-0.11702', '-0.049775', '0.001526', '-0.064243', '-0.060033',\n",
       "       '0.099257', '-0.11476', '-0.18558', '0.045222', '0.36369',\n",
       "       '0.078655', '0.0026155', '0.37286', '0.47996', '-0.2032',\n",
       "       '-0.37088', '0.68873', '-0.50702', '-0.12339', '-0.7473',\n",
       "       '-0.15392', '0.47928', '-0.51113', '-0.1744', '-0.0010949',\n",
       "       '-0.17066', '0.13301', '-0.97459', '0.44952', '-0.21822',\n",
       "       '-0.4853', '0.12379', '-0.12521', '-0.3585', '0.0077671',\n",
       "       '-0.16394', '0.1331', '-0.018957', '0.078365', '0.15956',\n",
       "       '0.22007', '0.2083', '-0.45311', '0.062373', '-0.18905', '-0.1078',\n",
       "       '0.41859', '0.043782', '-0.084046', '-0.023192', '0.20653',\n",
       "       '-0.13351', '-0.086689', '0.15332', '-0.22299', '-0.2856',\n",
       "       '0.2663', '0.030985', '0.089068', '0.12446', '-0.10969', '0.2539',\n",
       "       '0.045561', '-0.25968', '0.52391', '-0.35818', '0.1521',\n",
       "       '-0.30318', '-0.452', '0.40887', '-0.32797', '0.071868',\n",
       "       '-0.18051', '-0.14303', '-0.43196', '0.26489', '0.0065555',\n",
       "       '-0.33206', '-0.51717', '0.31031', '-0.021892', '-0.42763',\n",
       "       '-0.089138', '0.23109', '-0.72551', '-0.30852', '0.31625',\n",
       "       '-0.2414', '0.67413', '0.15109', '-0.61756', '0.47101', '0.096',\n",
       "       '0.24137', '-0.19271', '0.23127', '0.097419', '0.0060806',\n",
       "       '0.065882', '-0.15917', '-0.049049', '0.48555', '0.42522',\n",
       "       '0.1931', '-0.33896', '0.24325', '-0.31585', '0.071705',\n",
       "       '-0.31473', '0.24852', '0.33015', '-0.11509', '0.15554',\n",
       "       '-0.30897', '0.081276', '-0.25239', '0.39429', '-0.70939',\n",
       "       '-0.035006', '0.43547', '-0.09036', '-0.51861', '-0.47336',\n",
       "       '0.32614', '-0.19398', '0.13141', '0.63832', '0.19216', '0.23511',\n",
       "       '-0.3446', '-0.14881', '-0.34152', '0.053664', '0.57657',\n",
       "       '-0.084286', '0.65916', '-0.36212', '0.25502', '0.21766',\n",
       "       '0.41229', '-0.095384', '0.14312', '0.34689', '0.57112', '0.21812',\n",
       "       '0.26493', '0.46643', '0.2244', '0.12745', '-0.61129', '0.19823',\n",
       "       '0.31085', '-0.13135', '-0.72936', '-0.22244', '0.30406',\n",
       "       '0.56145', '-0.32136', '-0.023943', '0.23981', '0.16197',\n",
       "       '0.24358', '-0.12929', '0.24672', '0.43033', '-0.094143',\n",
       "       '0.34993', '0.014464', '-0.31511', '0.03881', '-0.084324',\n",
       "       '-0.24976', '0.12444', '-0.46186', '-0.087362', '0.36289',\n",
       "       '-0.15201', '-0.58263', '0.39734', '-0.5222', '-0.13725',\n",
       "       '0.46133', '-0.56852', '0.053333', '-0.32763', '1.3333', '0.13759',\n",
       "       '0.3337', '0.4845', '-0.34026', '0.10209', '-0.53855', '-0.21804',\n",
       "       '0.0070351', '-0.44067', '-0.29964', '-0.30904', '-0.4926',\n",
       "       '-0.13665', '-0.31936', '-0.06594', '-0.073131', '-0.037299',\n",
       "       '-0.17676', '-0.38624', '-0.37403', '0.35887', '0.2879', '0.119',\n",
       "       '-0.2653', '-0.097381', '0.015616', '-0.1358', '-0.32072',\n",
       "       '0.52225', '-0.1026', '0.2569', '0.19668', '0.31946', '0.31752',\n",
       "       '0.38601', '-0.32242', '0.010101', '0.56418', '0.18986',\n",
       "       '-0.11252', '-0.68244', '0.093768', '0.39945', '-0.1335',\n",
       "       '0.13784', '0.078892', '0.28101', '-0.01176', '0.33988',\n",
       "       '-0.28549', '0.2651', '-0.012508', '-0.28022', '-0.44001',\n",
       "       '-0.31615', '-0.24346', '-0.20195', '0.1478', '-0.037287',\n",
       "       '0.094966', '-0.7098', '-0.21835', '-0.060846', '-0.026851',\n",
       "       '0.10727', '0.53075', '0.27392', '-0.089232', '0.35236', '0.15331',\n",
       "       '-0.5554', '0.33857', '-0.35763', '0.087876', '0.023019',\n",
       "       '-0.53581', '-0.12216', '0.3624', '-0.08473', '0.095763',\n",
       "       '-0.12685', '-0.54834', '-0.050464', '0.045937', '-0.057361',\n",
       "       '-0.10337', '0.28224', '0.33506', '0.29008', '-0.39856',\n",
       "       '0.017586', '-0.20329', '-0.22494', '-0.021102', '0.74769',\n",
       "       '0.50657', '-0.74206', '-0.41123', '0.73799'], dtype='<U11')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[7]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

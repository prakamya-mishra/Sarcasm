{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset/train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[['parent_comment','comment','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a4357f6d8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.countplot(x='label',hue='label',data=df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>420656</th>\n",
       "      <td>The pilot could not be shot dead from the grou...</td>\n",
       "      <td>*cough* parachute</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822050</th>\n",
       "      <td>A bunch of guys go to Normandy to find Ryan, a...</td>\n",
       "      <td>WOAH SPOILER ALERT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848304</th>\n",
       "      <td>Found this Richmond ad browsing in r/wtf.</td>\n",
       "      <td>Think of the cleanup job, arrggg so gross</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537018</th>\n",
       "      <td>dibidy dont</td>\n",
       "      <td>dibididbidibidiidbidibdi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269018</th>\n",
       "      <td>The fence that encloses the smallest area in t...</td>\n",
       "      <td>Nuh uh, cuz the Earth is flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           parent_comment  \\\n",
       "420656  The pilot could not be shot dead from the grou...   \n",
       "822050  A bunch of guys go to Normandy to find Ryan, a...   \n",
       "848304          Found this Richmond ad browsing in r/wtf.   \n",
       "537018                                        dibidy dont   \n",
       "269018  The fence that encloses the smallest area in t...   \n",
       "\n",
       "                                          comment  label  \n",
       "420656                          *cough* parachute      0  \n",
       "822050                         WOAH SPOILER ALERT      1  \n",
       "848304  Think of the cleanup job, arrggg so gross      0  \n",
       "537018                   dibididbidibidiidbidibdi      0  \n",
       "269018              Nuh uh, cuz the Earth is flat      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    tokens_wo_stopwords = []\n",
    "    for i in range(0,len(tokens)):\n",
    "        if tokens[i].lower() not in stop_words:\n",
    "            tokens_wo_stopwords.append(tokens[i].lower())\n",
    "    return tokens_wo_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(token):\n",
    "    pos_tag = nltk.pos_tag([token])[0][1]\n",
    "    if pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(0,len(tokens)):\n",
    "        tokens[i] = lemmatizer.lemmatize(tokens[i],pos=str(get_pos_tag(tokens[i])))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dictionary(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in dictionary:\n",
    "            dictionary.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dictionary():\n",
    "    with open('data/processed/dictionary.txt','w') as file:\n",
    "        file.writelines(\"%s\\n\" % word for word in dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dictionary():\n",
    "    with open('data/processed/dictionary.txt','r') as file:\n",
    "        temp = file.read().splitlines()\n",
    "        for i in range(0,len(temp)):\n",
    "            dictionary.append(temp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    processed_sentence = re.sub(r'[^a-zA-Z]', ' ', sentence)\n",
    "    tokens_comment = word_tokenize(processed_sentence)\n",
    "    tokens_comment = remove_stopwords(tokens_comment)\n",
    "    tokens_comment = lemmatize(tokens_comment)\n",
    "    return tokens_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(dataset):\n",
    "    for index,row in dataset.iterrows():\n",
    "        tokens_comment = preprocess(str(row['parent_comment']) + \" \" + str(row['comment']))\n",
    "        add_to_dictionary(tokens_comment)\n",
    "    save_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_embeddings_dict():\n",
    "    starttime = time.time()\n",
    "    with open('data/processed/glove.6B.300d.txt','r') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            word_embedding = np.asarray(values[1:])\n",
    "            embeddings[word] = word_embedding\n",
    "    endtime = time.time()\n",
    "    print(\"Time taken to load embeddings:- \")\n",
    "    print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup(x,embedding_dim=300):\n",
    "    if(len(embeddings) == 0):\n",
    "        populate_embeddings_dict()\n",
    "    embedding = []\n",
    "    for i in range(0,len(x)):\n",
    "        if(x[i] in embeddings):\n",
    "            embedding.append(embeddings[x[i]])\n",
    "        else:\n",
    "            zero_arr = np.zeros(embedding_dim).tolist()\n",
    "            embedding.append(zero_arr)\n",
    "    return np.array(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/processed/dictionary.txt'):\n",
    "    starttime = time.time()\n",
    "    create_dictionary(df_new)\n",
    "    endtime = time.time()\n",
    "    print(\"Time to create dictionary\")\n",
    "    print(endtime - starttime)\n",
    "else:   \n",
    "    read_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67028"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load embeddings:- \n",
      "26.129017114639282\n"
     ]
    }
   ],
   "source": [
    "embeddings = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cough', 'parachute']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(df_new['comment'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\",trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elmo_embeddings(sess,tokens_input,tokens_length):\n",
    "    embeddings = elmo(inputs={\"tokens\": tokens_input,\"sequence_len\": tokens_length},signature='tokens',as_dict=True)[\"elmo\"]\n",
    "    return sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0412 21:44:46.317991 4423882176 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "tokens = preprocess(df_new['comment'].iloc[0])\n",
    "tokens_input = [tokens]\n",
    "tokens_length = [len(tokens)]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    elmo_embeddings = get_elmo_embeddings(sess,tokens_input,tokens_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 1024)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMO embeddings for 'cool'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.1345155 , -0.19303311,  0.17593093, ...,  0.47150755,\n",
       "        0.02966034,  0.46326378], dtype=float32)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"ELMO embeddings for 'cool'\")\n",
    "elmo_embeddings[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_elmo_embedding():\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        for index,rows in df_new.iterrows():\n",
    "            preprocessed_tokens = preprocess(rows['parent_comment'] + \" \" + rows['comment'])\n",
    "            embedding_tensor = elmo(preprocessed_tokens)\n",
    "            embeddings = sess.run(embedding_tensor)\n",
    "            #Rest of the network here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM():\n",
    "    \n",
    "    def __init__(self,num_classes,elmo_embed_size,embed_size,batch_size,epochs,max_length):\n",
    "        self.X = tf.placeholder(shape=[None,max_length,embed_size + elmo_embed_size],dtype=tf.float32)\n",
    "        self.y = tf.placeholder(shape=[None],dtype=tf.int64)\n",
    "        self.sequence_lengths = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.num_classes = num_classes\n",
    "        self.elmo_embed_size = elmo_embed_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = elmo_embed_size + embed_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.max_length = max_length\n",
    "        self.model()\n",
    "    \n",
    "    def model(self):\n",
    "        cell_fw = tf.contrib.rnn.LSTMCell(self.hidden_size,forget_bias=1.0,state_is_tuple=True,reuse=tf.get_variable_scope().reuse)\n",
    "        cell_bw = tf.contrib.rnn.LSTMCell(self.hidden_size,forget_bias=1.0,state_is_tuple=True,reuse=tf.get_variable_scope().reuse)\n",
    "        with tf.variable_scope('Bi-Directional-LSTM',reuse=tf.AUTO_REUSE):\n",
    "            output_vals,output_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw = cell_fw,\n",
    "            cell_bw = cell_bw,\n",
    "            inputs = self.X,\n",
    "            sequence_length = self.sequence_lengths,\n",
    "            dtype = tf.float32)\n",
    "        self.final_state = tf.concat([output_states[0].c,output_states[1].c],axis=1)\n",
    "        with tf.variable_scope('Softmax',reuse=tf.AUTO_REUSE):\n",
    "            self.softmax_w = tf.get_variable('softmax_w',shape=[2 * self.hidden_size,self.num_classes],initializer=tf.truncated_normal_initializer(),dtype=tf.float32)\n",
    "            self.softmax_b = tf.get_variable('softmax_b',shape=[self.num_classes],initializer=tf.constant_initializer(0.0),dtype=tf.float32)\n",
    "        self.logits = tf.matmul(self.final_state,self.softmax_w) + self.softmax_b\n",
    "        self.predictions = tf.argmax(tf.nn.softmax(self.logits),1,name='predictions')\n",
    "        self.cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y,logits=self.logits))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.predictions,self.y),tf.float32),name='accuracy')\n",
    "    \n",
    "    def train(self,X_train,X_test,y_train,y_test,sequence_lengths_train,sequence_lengths_test):\n",
    "        self.learning_rate = tf.train.exponential_decay(1e-3,global_step,15,1,staircase=True)\n",
    "        optimizer = tf.train.AdamOptimizer(lr=self.learning_rate)\n",
    "        train_step = optimizer.minimize(self.cost)\n",
    "        with tf.Graph().as_deafult():\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                for j in range(0,self.epochs):\n",
    "                    for i in range(0,X_train.shape[0],self.batch_size):\n",
    "                        X_train_batch = X_train[i * self.batch_size : min(i * (self.batch_size + 1) - 1,X_train.shape[0] - (i * self.batch_size) + 1)]\n",
    "                        y_train_batch = y_train[i * self.batch_size : min(i * (self.batch_size + 1) - 1,y_train.shape[0] - (i * self.batch_size) + 1)]\n",
    "                        sequence_lengths_batch = sequence_lengths_train[i * self.batch_size : min(i * (self.batch_size + 1) - 1,sequence_lengths_train.shape[0] - (i * self.batch_size) + 1)]\n",
    "                        fetches = {\n",
    "                            'cost': self.cost,\n",
    "                            'accuracy': self.accuracy,\n",
    "                            'predictions': self.predictions\n",
    "                        }\n",
    "                        feed_dict = {\n",
    "                            'X': X_train_batch,\n",
    "                            'y': y_train_batch,\n",
    "                            'sequence_lengths': sequence_lengths_batch\n",
    "                        }\n",
    "                        resp = sess.run(fetched,feed_dict)\n",
    "                    print(resp['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning required\n",
    "lstm = LSTM(2,1024,300,4000,5,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420656                            *cough* parachute\n",
      "822050                           WOAH SPOILER ALERT\n",
      "848304    Think of the cleanup job, arrggg so gross\n",
      "537018                     dibididbidibidiidbidibdi\n",
      "269018                Nuh uh, cuz the Earth is flat\n",
      "Name: comment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Remove nan here\n",
    "X = df_new['comment']\n",
    "y = df_new['label']\n",
    "X.reset_index()\n",
    "print(X.head())\n",
    "X_train,X_test,y_train,t_test = train_test_split(X,y,test_size=0.2,random_state=222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000,)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([533199, 415545, 815276, 709403, 275380, 305515, 487810, 173348,\n",
       "            872728, 927082,\n",
       "            ...\n",
       "            829183, 783248, 255816, 964228, 226890,  46567, 988743, 479766,\n",
       "            742475, 424065],\n",
       "           dtype='int64', length=80000)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([555725, 240091, 978765,  60492, 173865,  60919, 905619, 602610,\n",
       "            708643, 684005,\n",
       "            ...\n",
       "            607571, 335253,    156, 203365,  75947, 699733, 712162, 943482,\n",
       "            929689, 331835],\n",
       "           dtype='int64', length=20000)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tokens(tokens,max_length):\n",
    "    zeros = np.zeros(len(tokens[0]))\n",
    "    while len(tokens) < max_length:\n",
    "        tokens = np.vstack([tokens,zeros])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    max_length = 0\n",
    "    index = 0\n",
    "    for i in range(0,len(X_train.index)):\n",
    "        if(X_train[i:i+1][X_train.index[i]] is not nan):\n",
    "            preprocessed_tokens = preprocess(X_train[i:i+1][X_train.index[i]])\n",
    "            if(len(preprocessed_tokens) < 100):\n",
    "                if max_length < len(preprocessed_tokens):\n",
    "                    max_length = len(preprocessed_tokens)\n",
    "                    index = i\n",
    "    print(index)\n",
    "    print(preprocess(X_train[index:index+1][X_train.index[index]]))\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21596\n",
      "['v', 'jul', 'v', 'jul', 'v', 'jul', 'v', 'jun', 'v', 'jun', 'v', 'mar', 'v', 'feb', 'v', 'feb', 'v', 'feb', 'v', 'jan', 'v', 'dec', 'v', 'dec', 'v', 'sep', 'v', 'aug', 'v', 'jun', 'v', 'apr', 'v', 'mar', 'v', 'feb', 'v', 'feb', 'v', 'dec', 'v', 'dec', 'v', 'nov', 'v', 'sep', 'v', 'sep', 'v', 'aug', 'v', 'jul', 'v', 'jul', 'v', 'jun', 'v', 'may', 'v', 'apr', 'v', 'mar', 'v', 'mar', 'v', 'feb', 'v', 'feb', 'v', 'feb', 'v', 'jan', 'v', 'jan', 'v', 'jan', 'v', 'dec', 'v', 'dec', 'v', 'dec', 'v', 'dec', 'v', 'dec']\n",
      "Time to calculate max length:- \n",
      "117.96574902534485\n",
      "Max length:- \n",
      "86\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "max_length = get_max_length()\n",
    "endtime = time.time()\n",
    "print(\"Time to calculate max length:- \")\n",
    "print(endtime - starttime)\n",
    "print(\"Max length:- \")\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deep_contextualized_embeddings = []\n",
    "sequence_lengths_train = []\n",
    "elmo_tokens = []\n",
    "elmo_tokens_length = []\n",
    "elmo_embeddings_list = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    starttime = time.time()\n",
    "    for i in range(0,len(X_train.index)):\n",
    "        if(X_train[i:i+1][X_train.index[i]] is not nan):\n",
    "            preprocessed_tokens = preprocess(X_train[i:i+1][X_train.index[i]])\n",
    "            if(len(preprocessed_tokens) < 100):\n",
    "                sequence_lengths_train.append(len(preprocessed_tokens))\n",
    "                for j in range(len(preprocessed_tokens),max_length):\n",
    "                    preprocessed_tokens.append(\"<PAD>\")\n",
    "                #word_embedding = embedding_lookup(preprocessed_tokens)\n",
    "                #word_embedding = np.array(pad_tokens(word_embedding,max_length))\n",
    "                elmo_tokens.append(preprocessed_tokens)\n",
    "                elmo_tokens_length.append(len(preprocessed_tokens))\n",
    "                #deep_contextualized_embeddings.append(np.hstack([word_embedding,elmo_embedding]))\n",
    "                if (i +1 ) % 1000 == 0:\n",
    "                    elmo_embedding = get_elmo_embeddings(sess,np.array(elmo_tokens),np.array(elmo_tokens_length))\n",
    "                    for j in range(0,len(elmo_embedding)):\n",
    "                        deep_contextualized_embeddings.append(np.array(pad_tokens(elmo_embedding[j],max_length)))\n",
    "                    temp_arr = np.array(deep_contextualized_embeddings)\n",
    "                    print(temp_arr.shape)\n",
    "    endtime = time.time()\n",
    "    print(\"Total time to generate embeddings:- \")\n",
    "    print(endtime - starttime)\n",
    "    print(deep_contextualized_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

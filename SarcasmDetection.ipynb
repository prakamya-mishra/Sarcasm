{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashvatkedia/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0411 21:07:14.610810 4423882176 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset/train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[['parent_comment','comment','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a4357f6d8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.countplot(x='label',hue='label',data=df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>420656</th>\n",
       "      <td>The pilot could not be shot dead from the grou...</td>\n",
       "      <td>*cough* parachute</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822050</th>\n",
       "      <td>A bunch of guys go to Normandy to find Ryan, a...</td>\n",
       "      <td>WOAH SPOILER ALERT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848304</th>\n",
       "      <td>Found this Richmond ad browsing in r/wtf.</td>\n",
       "      <td>Think of the cleanup job, arrggg so gross</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537018</th>\n",
       "      <td>dibidy dont</td>\n",
       "      <td>dibididbidibidiidbidibdi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269018</th>\n",
       "      <td>The fence that encloses the smallest area in t...</td>\n",
       "      <td>Nuh uh, cuz the Earth is flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           parent_comment  \\\n",
       "420656  The pilot could not be shot dead from the grou...   \n",
       "822050  A bunch of guys go to Normandy to find Ryan, a...   \n",
       "848304          Found this Richmond ad browsing in r/wtf.   \n",
       "537018                                        dibidy dont   \n",
       "269018  The fence that encloses the smallest area in t...   \n",
       "\n",
       "                                          comment  label  \n",
       "420656                          *cough* parachute      0  \n",
       "822050                         WOAH SPOILER ALERT      1  \n",
       "848304  Think of the cleanup job, arrggg so gross      0  \n",
       "537018                   dibididbidibidiidbidibdi      0  \n",
       "269018              Nuh uh, cuz the Earth is flat      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    tokens_wo_stopwords = []\n",
    "    for i in range(0,len(tokens)):\n",
    "        if tokens[i].lower() not in stop_words:\n",
    "            tokens_wo_stopwords.append(tokens[i].lower())\n",
    "    return tokens_wo_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(token):\n",
    "    pos_tag = nltk.pos_tag([token])[0][1]\n",
    "    if pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(0,len(tokens)):\n",
    "        tokens[i] = lemmatizer.lemmatize(tokens[i],pos=str(get_pos_tag(tokens[i])))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dictionary(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in dictionary:\n",
    "            dictionary.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dictionary():\n",
    "    with open('data/processed/dictionary.txt','w') as file:\n",
    "        file.writelines(\"%s\\n\" % word for word in dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dictionary():\n",
    "    with open('data/processed/dictionary.txt','r') as file:\n",
    "        temp = file.read().splitlines()\n",
    "        for i in range(0,len(temp)):\n",
    "            dictionary.append(temp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    processed_sentence = re.sub(r'[^a-zA-Z]', ' ', sentence)\n",
    "    tokens_comment = word_tokenize(processed_sentence)\n",
    "    tokens_comment = remove_stopwords(tokens_comment)\n",
    "    tokens_comment = lemmatize(tokens_comment)\n",
    "    return tokens_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(dataset):\n",
    "    for index,row in dataset.iterrows():\n",
    "        tokens_comment = preprocess(str(row['parent_comment']) + \" \" + str(row['comment']))\n",
    "        add_to_dictionary(tokens_comment)\n",
    "    save_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_embeddings_dict():\n",
    "    starttime = time.time()\n",
    "    with open('data/processed/glove.6B.300d.txt','r') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            word_embedding = np.asarray(values[1:])\n",
    "            embeddings[word] = word_embedding\n",
    "    endtime = time.time()\n",
    "    print(\"Time taken to load embeddings:- \")\n",
    "    print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup(x,embedding_dim=300):\n",
    "    if(len(embeddings) == 0):\n",
    "        populate_embeddings_dict()\n",
    "    embedding = []\n",
    "    for i in range(0,len(x)):\n",
    "        if(x[i] in embeddings):\n",
    "            embedding.append(embeddings[x[i]])\n",
    "        else:\n",
    "            zero_arr = np.zeros(embedding_dim).tolist()\n",
    "            embedding.append(zero_arr)\n",
    "    embedding = np.array(embedding)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/processed/dictionary.txt'):\n",
    "    starttime = time.time()\n",
    "    create_dictionary(df_new)\n",
    "    endtime = time.time()\n",
    "    print(\"Time to create dictionary\")\n",
    "    print(endtime - starttime)\n",
    "else:   \n",
    "    read_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67028"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load embeddings:- \n",
      "26.129017114639282\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedding_lookup(preprocess(df_new['comment'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cough', 'parachute']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(df_new['comment'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\",trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elmo_embeddings(tokens_input,tokens_length):\n",
    "    embeddings = elmo(inputs={\"tokens\": tokens_input,\"sequence_len\": tokens_length},signature='tokens',as_dict=True)[\"elmo\"]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        return sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0411 21:25:04.964322 4423882176 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "tokens = preprocess(df_new['comment'].iloc[0])\n",
    "tokens_input = [tokens]\n",
    "tokens_length = [len(tokens)]\n",
    "elmo_embeddings = get_elmo_embeddings(tokens_input,tokens_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 1024)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMO embeddings for 'cool'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.1345155 , -0.19303311,  0.17593093, ...,  0.47150755,\n",
       "        0.02966034,  0.46326378], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"ELMO embeddings for 'cool'\")\n",
    "elmo_embeddings[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_elmo_embedding():\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        for index,rows in df_new.iterrows():\n",
    "            preprocessed_tokens = preprocess(rows['parent_comment'] + \" \" + rows['comment'])\n",
    "            embedding_tensor = elmo(preprocessed_tokens)\n",
    "            embeddings = sess.run(embedding_tensor)\n",
    "            #Rest of the network here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM():\n",
    "    \n",
    "    def __init__(self,num_classes,elmo_embed_size,embed_size,batch_size,epochs,max_length):\n",
    "        self.X = tf.placeholder(shape=[None,max_length,embed_size + elmo_embed_size],dtype=tf.float32)\n",
    "        self.y = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.sequence_lengths = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.num_classes = num_classes\n",
    "        self.elmo_embed_size = elmo_embed_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = elmo_embed_size + embed_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.max_length = max_length\n",
    "        self.model()\n",
    "    \n",
    "    def model(self):\n",
    "        cell_fw = tf.contrib.rnn.LSTMCell(self.hidden_size,forget_bias=1.0,state_is_tuple=True,reuse=tf.get_variable_scope().reuse)\n",
    "        cell_bw = tf.contrib.rnn.LSTMCell(self.hidden_size,forget_bias=1.0,state_is_tuple=True,reuse=tf.get_variable_scope().reuse)\n",
    "        with tf.variable_scope('Bi-Directional-LSTM',reuse=tf.AUTO_REUSE):\n",
    "            output_vals,output_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw = cell_fw,\n",
    "            cell_bw = cell_bw,\n",
    "            inputs = self.X,\n",
    "            sequence_length = self.sequence_lengths,\n",
    "            dtype = tf.float32)\n",
    "        self.final_state = tf.concat([output_states[0].c,output_states[1].c],axis=1)\n",
    "        print(self.final_state.shape)\n",
    "        with tf.variable_scope('Softmax',reuse=tf.AUTO_REUSE):\n",
    "            self.softmax_w = tf.get_variable('softmax_w',shape=[2 * self.hidden_size,self.num_classes],initializer=tf.truncated_normal_initializer(),dtype=tf.float32)\n",
    "            self.softmax_b = tf.get_variable('softmax_b',shape=[self.num_classes],initializer=tf.constant_initializer(0.0),dtype=tf.float32)\n",
    "        self.logits = tf.matmul(self.final_state,self.softmax_w) + self.softmax_b\n",
    "        print(self.logits.shape)\n",
    "        self.predictions = tf.argmax(tf.nn.softmax(self.logits),1,name='predictions')\n",
    "        self.cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y,logits=self.logits))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.predictions,self.y),tf.float32),name='accuracy')\n",
    "    \n",
    "    def train(self,X_train,X_test,y_train,y_test,sequence_lengths_train,sequence_lengths_test):\n",
    "        self.learning_rate = tf.train.exponential_decay(1e-3,global_step,15,1,staircase=True)\n",
    "        optimizer = tf.train.AdamOptimizer(lr=self.learning_rate)\n",
    "        train_step = optimizer.minimize(self.cost)\n",
    "        with tf.Graph().as_deafult():\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                for j in range(0,self.epochs):\n",
    "                    for i in range(0,X_train.shape[0],self.batch_size):\n",
    "                        X_train_batch = X_train[i * self.batch_size : min(i * (self.batch_size + 1) - 1,X_train.shape[0] - (i * self.batch_size) + 1)]\n",
    "                        y_train_batch = y_train[i * self.batch_size : min(i * (self.batch_size + 1) - 1,y_train.shape[0] - (i * self.batch_size) + 1)]\n",
    "                        sequence_lengths_batch = sequence_lengths_train[i * self.batch_size : min(i * (self.batch_size + 1) - 1,sequence_lengths_train.shape[0] - (i * self.batch_size) + 1)]\n",
    "                        fetches = {\n",
    "                            'cost': self.cost,\n",
    "                            'accuracy': self.accuracy,\n",
    "                            'predictions': self.predictions\n",
    "                        }\n",
    "                        feed_dict = {\n",
    "                            'X': X_train_batch,\n",
    "                            'y': y_train_batch,\n",
    "                            'sequence_lengths': sequence_lengths_batch\n",
    "                        }\n",
    "                        resp = sess.run(fetched,feed_dict)\n",
    "                    print(resp['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2648)\n",
      "(?, 2)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Input 'y' of 'Equal' Op has type int32 that does not match type int64 of argument 'x'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    511\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[0;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m         (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[1;32m    984\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype int64 for Tensor with dtype int32: 'Tensor(\"Placeholder_105:0\", shape=(?,), dtype=int32)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-59a10d14c833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-109-bc008fe0b049>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes, elmo_embed_size, embed_size, batch_size, epochs, max_length)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-109-bc008fe0b049>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_lengths_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_lengths_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mequal\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   2732\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 2734\u001b[0;31m         \"Equal\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m   2735\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    544\u001b[0m                   \u001b[0;34m\"%s type %s of argument '%s'.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n\u001b[0;32m--> 546\u001b[0;31m                    inferred_from[input_arg.type_attr]))\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m           \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input 'y' of 'Equal' Op has type int32 that does not match type int64 of argument 'x'."
     ]
    }
   ],
   "source": [
    "lstm = LSTM(2,1024,300,4000,5,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
